{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚨 PIPELINE EXPLÍCITO: RASTREAMENTO COMPLETO DE FEATURES\n",
    "\n",
    "## Estratégia de Correção:\n",
    "1. **Pipeline Explícito**: Cada dataframe tem nome único e descritivo\n",
    "2. **Auditoria de Penhascos**: Identificar exatamente onde features são perdidas\n",
    "3. **Checkpoints com .info()**: Inspeção detalhada em cada etapa\n",
    "4. **Preservação Rigorosa**: Garantir que nenhuma feature seja perdida\n",
    "\n",
    "## Nomenclatura do Pipeline:\n",
    "- `df_step01_raw`: Dados brutos carregados\n",
    "- `df_step02_processed`: Dados processados básicos\n",
    "- `df_step03_features`: Com features técnicas criadas\n",
    "- `df_step04_selected`: Após seleção de features\n",
    "- `df_step05_temporal`: Após correção temporal\n",
    "- `df_step06_candlestick`: Com padrões de candlestick\n",
    "- `df_step07_final`: Dataset final para modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Função de auditoria rigorosa criada\n",
      "📋 Pronta para rastrear cada etapa do pipeline\n"
     ]
    }
   ],
   "source": [
    "# FUNÇÃO DE AUDITORIA RIGOROSA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def audit_pipeline_step(df, step_name, expected_min_cols=None, previous_df=None):\n",
    "    \"\"\"\n",
    "    Auditoria rigorosa de cada etapa do pipeline\n",
    "    Identifica exatamente onde features são perdidas\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🔍 AUDITORIA: {step_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Informações básicas\n",
    "    print(f\"📊 INFORMAÇÕES BÁSICAS:\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Memória: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Usar .info() para inspeção detalhada\n",
    "    print(f\"\\n📋 DETALHES DAS COLUNAS (.info()):\")\n",
    "    df.info()\n",
    "    \n",
    "    # Categorização de features\n",
    "    ohlc_cols = [col for col in df.columns if col in ['Abertura', 'Máxima', 'Mínima', 'Último', 'Volume']]\n",
    "    temporal_cols = [col for col in df.columns if any(x in col.lower() for x in ['data', 'day', 'month', 'quarter', 'year'])]\n",
    "    technical_cols = [col for col in df.columns if any(x in col.lower() for x in ['ma_', 'bb_', 'rsi', 'atr', 'volatility', 'macd'])]\n",
    "    shifted_cols = [col for col in df.columns if col.endswith('_shifted')]\n",
    "    candlestick_cols = [col for col in df.columns if any(x in col.lower() for x in ['doji', 'hammer', 'engulf', 'marubozu', 'shooting', '_prev'])]\n",
    "    target_cols = [col for col in df.columns if 'target' in col.lower()]\n",
    "    other_cols = [col for col in df.columns if col not in ohlc_cols + temporal_cols + technical_cols + shifted_cols + candlestick_cols + target_cols]\n",
    "    \n",
    "    print(f\"\\n📂 CATEGORIZAÇÃO DE FEATURES:\")\n",
    "    print(f\"   🏢 OHLC/Volume ({len(ohlc_cols)}): {ohlc_cols}\")\n",
    "    print(f\"   📅 Temporais ({len(temporal_cols)}): {temporal_cols}\")\n",
    "    print(f\"   📈 Técnicas ({len(technical_cols)}): {technical_cols[:5]}{'...' if len(technical_cols) > 5 else ''}\")\n",
    "    print(f\"   ⏰ Shifted ({len(shifted_cols)}): {shifted_cols[:5]}{'...' if len(shifted_cols) > 5 else ''}\")\n",
    "    print(f\"   🕯️ Candlestick ({len(candlestick_cols)}): {candlestick_cols[:5]}{'...' if len(candlestick_cols) > 5 else ''}\")\n",
    "    print(f\"   🎯 Target ({len(target_cols)}): {target_cols}\")\n",
    "    print(f\"   ❓ Outras ({len(other_cols)}): {other_cols[:5]}{'...' if len(other_cols) > 5 else ''}\")\n",
    "    \n",
    "    # Verificar se há perda de features\n",
    "    if previous_df is not None:\n",
    "        print(f\"\\n🚨 ANÁLISE DE MUDANÇAS:\")\n",
    "        prev_cols = set(previous_df.columns)\n",
    "        curr_cols = set(df.columns)\n",
    "        \n",
    "        lost_features = prev_cols - curr_cols\n",
    "        gained_features = curr_cols - prev_cols\n",
    "        \n",
    "        print(f\"   📉 Features PERDIDAS ({len(lost_features)}):\")\n",
    "        if lost_features:\n",
    "            for feature in sorted(lost_features):\n",
    "                print(f\"      ❌ {feature}\")\n",
    "        else:\n",
    "            print(f\"      ✅ Nenhuma feature perdida\")\n",
    "        \n",
    "        print(f\"   📈 Features GANHAS ({len(gained_features)}):\")\n",
    "        if gained_features:\n",
    "            for feature in sorted(gained_features):\n",
    "                print(f\"      ✅ {feature}\")\n",
    "        else:\n",
    "            print(f\"      ➖ Nenhuma feature ganha\")\n",
    "        \n",
    "        net_change = len(curr_cols) - len(prev_cols)\n",
    "        print(f\"   📊 Mudança líquida: {net_change:+d} features\")\n",
    "        \n",
    "        # ALERTA para perdas significativas\n",
    "        if len(lost_features) > 5:\n",
    "            print(f\"\\n🚨 ALERTA: PERDA SIGNIFICATIVA DE FEATURES!\")\n",
    "            print(f\"   {len(lost_features)} features foram perdidas nesta etapa\")\n",
    "            print(f\"   Isso pode indicar um 'PENHASCO' no pipeline\")\n",
    "    \n",
    "    # Verificar expectativas\n",
    "    if expected_min_cols and len(df.columns) < expected_min_cols:\n",
    "        print(f\"\\n⚠️ AVISO: Menos colunas que esperado\")\n",
    "        print(f\"   Esperado: >= {expected_min_cols}\")\n",
    "        print(f\"   Atual: {len(df.columns)}\")\n",
    "    \n",
    "    # Verificar qualidade dos dados\n",
    "    print(f\"\\n🔍 QUALIDADE DOS DADOS:\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    cols_with_nulls = null_counts[null_counts > 0]\n",
    "    print(f\"   Colunas com valores ausentes: {len(cols_with_nulls)}\")\n",
    "    if len(cols_with_nulls) > 0:\n",
    "        print(f\"   Top 5 com mais NaNs:\")\n",
    "        for col, count in cols_with_nulls.head().items():\n",
    "            print(f\"      {col}: {count} ({count/len(df):.1%})\")\n",
    "    \n",
    "    print(f\"\\n✅ Auditoria de {step_name} concluída\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return {\n",
    "        'step_name': step_name,\n",
    "        'shape': df.shape,\n",
    "        'columns': list(df.columns),\n",
    "        'ohlc_count': len(ohlc_cols),\n",
    "        'technical_count': len(technical_cols),\n",
    "        'shifted_count': len(shifted_cols),\n",
    "        'candlestick_count': len(candlestick_cols)\n",
    "    }\n",
    "\n",
    "print(\"✅ Função de auditoria rigorosa criada\")\n",
    "print(\"📋 Pronta para rastrear cada etapa do pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 01: CARREGAMENTO DOS DADOS BRUTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 INICIANDO PIPELINE EXPLÍCITO\n",
      "📂 STEP 01: Carregando dados brutos...\n",
      "\n",
      "============================================================\n",
      "🔍 AUDITORIA: STEP 01 - DADOS BRUTOS\n",
      "============================================================\n",
      "📊 INFORMAÇÕES BÁSICAS:\n",
      "   Shape: (3592, 7)\n",
      "   Memória: 0.68 MB\n",
      "\n",
      "📋 DETALHES DAS COLUNAS (.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3592 entries, 0 to 3591\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Data      3592 non-null   object \n",
      " 1   Último    3592 non-null   float64\n",
      " 2   Abertura  3592 non-null   float64\n",
      " 3   Máxima    3592 non-null   float64\n",
      " 4   Mínima    3592 non-null   float64\n",
      " 5   Vol.      3591 non-null   object \n",
      " 6   Var%      3592 non-null   object \n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 196.6+ KB\n",
      "\n",
      "📂 CATEGORIZAÇÃO DE FEATURES:\n",
      "   🏢 OHLC/Volume (4): ['Último', 'Abertura', 'Máxima', 'Mínima']\n",
      "   📅 Temporais (1): ['Data']\n",
      "   📈 Técnicas (0): []\n",
      "   ⏰ Shifted (0): []\n",
      "   🕯️ Candlestick (0): []\n",
      "   🎯 Target (0): []\n",
      "   ❓ Outras (2): ['Vol.', 'Var%']\n",
      "\n",
      "🔍 QUALIDADE DOS DADOS:\n",
      "   Colunas com valores ausentes: 1\n",
      "   Top 5 com mais NaNs:\n",
      "      Vol.: 1 (0.0%)\n",
      "\n",
      "✅ Auditoria de STEP 01 - Dados Brutos concluída\n",
      "============================================================\n",
      "\n",
      "📋 RESUMO STEP 01:\n",
      "   ✅ Dados carregados: (3592, 7)\n",
      "   📊 Colunas originais: ['Data', 'Último', 'Abertura', 'Máxima', 'Mínima', 'Vol.', 'Var%']\n"
     ]
    }
   ],
   "source": [
    "# STEP 01: CARREGAMENTO DOS DADOS BRUTOS\n",
    "print(\"🚀 INICIANDO PIPELINE EXPLÍCITO\")\n",
    "print(\"📂 STEP 01: Carregando dados brutos...\")\n",
    "\n",
    "# Carregar dados brutos\n",
    "df_step01_raw = pd.read_csv('Dados Históricos - Ibovespa.csv', encoding='utf-8')\n",
    "\n",
    "# Auditoria do carregamento\n",
    "audit_step01 = audit_pipeline_step(df_step01_raw, \"STEP 01 - Dados Brutos\")\n",
    "\n",
    "print(f\"\\n📋 RESUMO STEP 01:\")\n",
    "print(f\"   ✅ Dados carregados: {df_step01_raw.shape}\")\n",
    "print(f\"   📊 Colunas originais: {list(df_step01_raw.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 02: PROCESSAMENTO BÁSICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 STEP 02: Processamento básico...\n",
      "\n",
      "============================================================\n",
      "🔍 AUDITORIA: STEP 02 - PROCESSAMENTO BÁSICO\n",
      "============================================================\n",
      "📊 INFORMAÇÕES BÁSICAS:\n",
      "   Shape: (3591, 10)\n",
      "   Memória: 0.59 MB\n",
      "\n",
      "📋 DETALHES DAS COLUNAS (.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3591 entries, 0 to 3590\n",
      "Data columns (total 10 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   Data      3591 non-null   datetime64[ns]\n",
      " 1   Último    3591 non-null   float64       \n",
      " 2   Abertura  3591 non-null   float64       \n",
      " 3   Máxima    3591 non-null   float64       \n",
      " 4   Mínima    3591 non-null   float64       \n",
      " 5   Vol.      3591 non-null   object        \n",
      " 6   Var%      3591 non-null   object        \n",
      " 7   Volume    3591 non-null   float64       \n",
      " 8   Variacao  3591 non-null   float64       \n",
      " 9   Target    3591 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(6), int64(1), object(2)\n",
      "memory usage: 280.7+ KB\n",
      "\n",
      "📂 CATEGORIZAÇÃO DE FEATURES:\n",
      "   🏢 OHLC/Volume (5): ['Último', 'Abertura', 'Máxima', 'Mínima', 'Volume']\n",
      "   📅 Temporais (1): ['Data']\n",
      "   📈 Técnicas (0): []\n",
      "   ⏰ Shifted (0): []\n",
      "   🕯️ Candlestick (0): []\n",
      "   🎯 Target (1): ['Target']\n",
      "   ❓ Outras (3): ['Vol.', 'Var%', 'Variacao']\n",
      "\n",
      "🚨 ANÁLISE DE MUDANÇAS:\n",
      "   📉 Features PERDIDAS (0):\n",
      "      ✅ Nenhuma feature perdida\n",
      "   📈 Features GANHAS (3):\n",
      "      ✅ Target\n",
      "      ✅ Variacao\n",
      "      ✅ Volume\n",
      "   📊 Mudança líquida: +3 features\n",
      "\n",
      "🔍 QUALIDADE DOS DADOS:\n",
      "   Colunas com valores ausentes: 0\n",
      "\n",
      "✅ Auditoria de STEP 02 - Processamento Básico concluída\n",
      "============================================================\n",
      "\n",
      "📋 RESUMO STEP 02:\n",
      "   ✅ Target criado corretamente\n",
      "   📊 Shape após processamento: (3591, 10)\n",
      "   🎯 Distribuição do target:\n",
      "Target\n",
      "1    0.510721\n",
      "0    0.489279\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# STEP 02: PROCESSAMENTO BÁSICO\n",
    "print(\"\\n📂 STEP 02: Processamento básico...\")\n",
    "\n",
    "# Criar cópia para processamento\n",
    "df_step02_processed = df_step01_raw.copy()\n",
    "\n",
    "# Processamento de data\n",
    "df_step02_processed['Data'] = pd.to_datetime(df_step02_processed['Data'], format='%d.%m.%Y')\n",
    "df_step02_processed = df_step02_processed.sort_values('Data').reset_index(drop=True)\n",
    "\n",
    "# Tratar valores ausentes\n",
    "df_step02_processed['Vol.'] = df_step02_processed['Vol.'].fillna(method='ffill')\n",
    "\n",
    "# Converter Volume para numérico\n",
    "def converter_volume(vol_str):\n",
    "    if pd.isna(vol_str): return np.nan\n",
    "    vol_str = str(vol_str).replace(',', '.')\n",
    "    if 'B' in vol_str: return float(vol_str.replace('B', '')) * 1e9\n",
    "    elif 'M' in vol_str: return float(vol_str.replace('M', '')) * 1e6\n",
    "    elif 'K' in vol_str: return float(vol_str.replace('K', '')) * 1e3\n",
    "    return float(vol_str)\n",
    "\n",
    "df_step02_processed['Volume'] = df_step02_processed['Vol.'].apply(converter_volume)\n",
    "df_step02_processed['Variacao'] = df_step02_processed['Var%'].str.replace('%', '').str.replace(',', '.').astype(float) / 100\n",
    "\n",
    "# Criar target\n",
    "df_step02_processed['Target'] = (df_step02_processed['Variacao'].shift(-1) > 0).astype(int)\n",
    "df_step02_processed = df_step02_processed[:-1].copy()  # Remove última linha\n",
    "\n",
    "# Auditoria do processamento\n",
    "audit_step02 = audit_pipeline_step(\n",
    "    df_step02_processed, \n",
    "    \"STEP 02 - Processamento Básico\", \n",
    "    expected_min_cols=len(df_step01_raw.columns),\n",
    "    previous_df=df_step01_raw\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 RESUMO STEP 02:\")\n",
    "print(f\"   ✅ Target criado corretamente\")\n",
    "print(f\"   📊 Shape após processamento: {df_step02_processed.shape}\")\n",
    "print(f\"   🎯 Distribuição do target:\")\n",
    "print(df_step02_processed['Target'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 03: CRIAÇÃO DE FEATURES TÉCNICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 STEP 03: Criando features técnicas...\n",
      "📈 Criando indicadores técnicos...\n",
      "\n",
      "============================================================\n",
      "🔍 AUDITORIA: STEP 03 - FEATURES TÉCNICAS\n",
      "============================================================\n",
      "📊 INFORMAÇÕES BÁSICAS:\n",
      "   Shape: (3591, 44)\n",
      "   Memória: 1.47 MB\n",
      "\n",
      "📋 DETALHES DAS COLUNAS (.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3591 entries, 0 to 3590\n",
      "Data columns (total 44 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   Data             3591 non-null   datetime64[ns]\n",
      " 1   Último           3591 non-null   float64       \n",
      " 2   Abertura         3591 non-null   float64       \n",
      " 3   Máxima           3591 non-null   float64       \n",
      " 4   Mínima           3591 non-null   float64       \n",
      " 5   Vol.             3591 non-null   object        \n",
      " 6   Var%             3591 non-null   object        \n",
      " 7   Volume           3591 non-null   float64       \n",
      " 8   Variacao         3591 non-null   float64       \n",
      " 9   Target           3591 non-null   int64         \n",
      " 10  MA_5             3587 non-null   float64       \n",
      " 11  MA_10            3582 non-null   float64       \n",
      " 12  MA_20            3572 non-null   float64       \n",
      " 13  MA_50            3542 non-null   float64       \n",
      " 14  BB_Middle        3572 non-null   float64       \n",
      " 15  BB_Upper         3572 non-null   float64       \n",
      " 16  BB_Lower         3572 non-null   float64       \n",
      " 17  BB_Width         3572 non-null   float64       \n",
      " 18  BB_Position      3572 non-null   float64       \n",
      " 19  RSI              3578 non-null   float64       \n",
      " 20  MACD             3591 non-null   float64       \n",
      " 21  Signal_Line      3591 non-null   float64       \n",
      " 22  high_low         3591 non-null   float64       \n",
      " 23  high_close_prev  3590 non-null   float64       \n",
      " 24  low_close_prev   3590 non-null   float64       \n",
      " 25  true_range       3591 non-null   float64       \n",
      " 26  atr_5            3587 non-null   float64       \n",
      " 27  atr_10           3582 non-null   float64       \n",
      " 28  atr_20           3572 non-null   float64       \n",
      " 29  returns          3590 non-null   float64       \n",
      " 30  volatility_5     3586 non-null   float64       \n",
      " 31  volatility_10    3581 non-null   float64       \n",
      " 32  volatility_20    3571 non-null   float64       \n",
      " 33  Price_Range      3591 non-null   float64       \n",
      " 34  Price_Position   3591 non-null   float64       \n",
      " 35  Gap              3590 non-null   float64       \n",
      " 36  hl_close_ratio   3591 non-null   float64       \n",
      " 37  day_of_week      3591 non-null   int32         \n",
      " 38  month            3591 non-null   int32         \n",
      " 39  quarter          3591 non-null   int32         \n",
      " 40  year             3591 non-null   int32         \n",
      " 41  is_month_start   3591 non-null   int64         \n",
      " 42  is_month_end     3591 non-null   int64         \n",
      " 43  is_quarter_end   3591 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(33), int32(4), int64(4), object(2)\n",
      "memory usage: 1.2+ MB\n",
      "\n",
      "📂 CATEGORIZAÇÃO DE FEATURES:\n",
      "   🏢 OHLC/Volume (5): ['Último', 'Abertura', 'Máxima', 'Mínima', 'Volume']\n",
      "   📅 Temporais (8): ['Data', 'day_of_week', 'month', 'quarter', 'year', 'is_month_start', 'is_month_end', 'is_quarter_end']\n",
      "   📈 Técnicas (17): ['MA_5', 'MA_10', 'MA_20', 'MA_50', 'BB_Middle']...\n",
      "   ⏰ Shifted (0): []\n",
      "   🕯️ Candlestick (2): ['high_close_prev', 'low_close_prev']\n",
      "   🎯 Target (1): ['Target']\n",
      "   ❓ Outras (11): ['Vol.', 'Var%', 'Variacao', 'Signal_Line', 'high_low']...\n",
      "\n",
      "🚨 ANÁLISE DE MUDANÇAS:\n",
      "   📉 Features PERDIDAS (0):\n",
      "      ✅ Nenhuma feature perdida\n",
      "   📈 Features GANHAS (34):\n",
      "      ✅ BB_Lower\n",
      "      ✅ BB_Middle\n",
      "      ✅ BB_Position\n",
      "      ✅ BB_Upper\n",
      "      ✅ BB_Width\n",
      "      ✅ Gap\n",
      "      ✅ MACD\n",
      "      ✅ MA_10\n",
      "      ✅ MA_20\n",
      "      ✅ MA_5\n",
      "      ✅ MA_50\n",
      "      ✅ Price_Position\n",
      "      ✅ Price_Range\n",
      "      ✅ RSI\n",
      "      ✅ Signal_Line\n",
      "      ✅ atr_10\n",
      "      ✅ atr_20\n",
      "      ✅ atr_5\n",
      "      ✅ day_of_week\n",
      "      ✅ high_close_prev\n",
      "      ✅ high_low\n",
      "      ✅ hl_close_ratio\n",
      "      ✅ is_month_end\n",
      "      ✅ is_month_start\n",
      "      ✅ is_quarter_end\n",
      "      ✅ low_close_prev\n",
      "      ✅ month\n",
      "      ✅ quarter\n",
      "      ✅ returns\n",
      "      ✅ true_range\n",
      "      ✅ volatility_10\n",
      "      ✅ volatility_20\n",
      "      ✅ volatility_5\n",
      "      ✅ year\n",
      "   📊 Mudança líquida: +34 features\n",
      "\n",
      "🔍 QUALIDADE DOS DADOS:\n",
      "   Colunas com valores ausentes: 20\n",
      "   Top 5 com mais NaNs:\n",
      "      MA_5: 4 (0.1%)\n",
      "      MA_10: 9 (0.3%)\n",
      "      MA_20: 19 (0.5%)\n",
      "      MA_50: 49 (1.4%)\n",
      "      BB_Middle: 19 (0.5%)\n",
      "\n",
      "✅ Auditoria de STEP 03 - Features Técnicas concluída\n",
      "============================================================\n",
      "\n",
      "📋 RESUMO STEP 03:\n",
      "   ✅ Features técnicas criadas: 34\n",
      "   📊 Total de features: 44\n",
      "   🎯 Shape final: (3591, 44)\n"
     ]
    }
   ],
   "source": [
    "# STEP 03: CRIAÇÃO DE FEATURES TÉCNICAS\n",
    "print(\"\\n📂 STEP 03: Criando features técnicas...\")\n",
    "\n",
    "# Criar cópia para features\n",
    "df_step03_features = df_step02_processed.copy()\n",
    "\n",
    "print(\"📈 Criando indicadores técnicos...\")\n",
    "\n",
    "# Médias móveis\n",
    "for periodo in [5, 10, 20, 50]:\n",
    "    df_step03_features[f'MA_{periodo}'] = df_step03_features['Último'].rolling(window=periodo).mean()\n",
    "\n",
    "# Bandas de Bollinger\n",
    "df_step03_features['BB_Middle'] = df_step03_features['Último'].rolling(window=20).mean()\n",
    "bb_std = df_step03_features['Último'].rolling(window=20).std()\n",
    "df_step03_features['BB_Upper'] = df_step03_features['BB_Middle'] + (bb_std * 2)\n",
    "df_step03_features['BB_Lower'] = df_step03_features['BB_Middle'] - (bb_std * 2)\n",
    "df_step03_features['BB_Width'] = df_step03_features['BB_Upper'] - df_step03_features['BB_Lower']\n",
    "df_step03_features['BB_Position'] = (df_step03_features['Último'] - df_step03_features['BB_Lower']) / df_step03_features['BB_Width']\n",
    "\n",
    "# RSI\n",
    "def calculate_rsi(prices, window=14):\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df_step03_features['RSI'] = calculate_rsi(df_step03_features['Último'])\n",
    "\n",
    "# MACD\n",
    "ema_12 = df_step03_features['Último'].ewm(span=12).mean()\n",
    "ema_26 = df_step03_features['Último'].ewm(span=26).mean()\n",
    "df_step03_features['MACD'] = ema_12 - ema_26\n",
    "df_step03_features['Signal_Line'] = df_step03_features['MACD'].ewm(span=9).mean()\n",
    "\n",
    "# ATR (Average True Range)\n",
    "df_step03_features['high_low'] = df_step03_features['Máxima'] - df_step03_features['Mínima']\n",
    "df_step03_features['high_close_prev'] = abs(df_step03_features['Máxima'] - df_step03_features['Último'].shift(1))\n",
    "df_step03_features['low_close_prev'] = abs(df_step03_features['Mínima'] - df_step03_features['Último'].shift(1))\n",
    "df_step03_features['true_range'] = df_step03_features[['high_low', 'high_close_prev', 'low_close_prev']].max(axis=1)\n",
    "\n",
    "for periodo in [5, 10, 20]:\n",
    "    df_step03_features[f'atr_{periodo}'] = df_step03_features['true_range'].rolling(window=periodo).mean()\n",
    "\n",
    "# Volatilidade\n",
    "df_step03_features['returns'] = df_step03_features['Último'].pct_change()\n",
    "for periodo in [5, 10, 20]:\n",
    "    df_step03_features[f'volatility_{periodo}'] = df_step03_features['returns'].rolling(window=periodo).std()\n",
    "\n",
    "# Features de preço\n",
    "df_step03_features['Price_Range'] = df_step03_features['Máxima'] - df_step03_features['Mínima']\n",
    "df_step03_features['Price_Position'] = (df_step03_features['Último'] - df_step03_features['Mínima']) / df_step03_features['Price_Range']\n",
    "df_step03_features['Gap'] = df_step03_features['Abertura'] - df_step03_features['Último'].shift(1)\n",
    "df_step03_features['hl_close_ratio'] = (df_step03_features['Máxima'] - df_step03_features['Mínima']) / df_step03_features['Último']\n",
    "\n",
    "# Features temporais\n",
    "df_step03_features['day_of_week'] = df_step03_features['Data'].dt.dayofweek\n",
    "df_step03_features['month'] = df_step03_features['Data'].dt.month\n",
    "df_step03_features['quarter'] = df_step03_features['Data'].dt.quarter\n",
    "df_step03_features['year'] = df_step03_features['Data'].dt.year\n",
    "df_step03_features['is_month_start'] = (df_step03_features['Data'].dt.day <= 5).astype(int)\n",
    "df_step03_features['is_month_end'] = (df_step03_features['Data'].dt.day >= 25).astype(int)\n",
    "df_step03_features['is_quarter_end'] = df_step03_features['Data'].dt.is_quarter_end.astype(int)\n",
    "\n",
    "# Auditoria da criação de features\n",
    "audit_step03 = audit_pipeline_step(\n",
    "    df_step03_features, \n",
    "    \"STEP 03 - Features Técnicas\", \n",
    "    expected_min_cols=30,  # Esperamos pelo menos 30 features\n",
    "    previous_df=df_step02_processed\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 RESUMO STEP 03:\")\n",
    "print(f\"   ✅ Features técnicas criadas: {df_step03_features.shape[1] - df_step02_processed.shape[1]}\")\n",
    "print(f\"   📊 Total de features: {df_step03_features.shape[1]}\")\n",
    "print(f\"   🎯 Shape final: {df_step03_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 04: SELEÇÃO DE FEATURES (PONTO CRÍTICO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 STEP 04: Seleção de features (PONTO CRÍTICO)...\n",
      "🚨 ATENÇÃO: Esta é uma área comum para 'PENHASCOS' de features!\n",
      "\n",
      "🔍 CHECKPOINT ANTES DA SELEÇÃO:\n",
      "   Features disponíveis: 44\n",
      "   Primeiras 10: ['Data', 'Último', 'Abertura', 'Máxima', 'Mínima', 'Vol.', 'Var%', 'Volume', 'Variacao', 'Target']\n",
      "   Últimas 10: ['Price_Position', 'Gap', 'hl_close_ratio', 'day_of_week', 'month', 'quarter', 'year', 'is_month_start', 'is_month_end', 'is_quarter_end']\n",
      "\n",
      "🗑️ Removendo apenas colunas auxiliares: ['Vol.', 'Var%', 'high_low', 'BB_Middle']\n",
      "\n",
      "✅ Todas as features importantes preservadas\n",
      "\n",
      "============================================================\n",
      "🔍 AUDITORIA: STEP 04 - SELEÇÃO DE FEATURES\n",
      "============================================================\n",
      "📊 INFORMAÇÕES BÁSICAS:\n",
      "   Shape: (3591, 40)\n",
      "   Memória: 1.04 MB\n",
      "\n",
      "📋 DETALHES DAS COLUNAS (.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3591 entries, 0 to 3590\n",
      "Data columns (total 40 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   Data             3591 non-null   datetime64[ns]\n",
      " 1   Último           3591 non-null   float64       \n",
      " 2   Abertura         3591 non-null   float64       \n",
      " 3   Máxima           3591 non-null   float64       \n",
      " 4   Mínima           3591 non-null   float64       \n",
      " 5   Volume           3591 non-null   float64       \n",
      " 6   Variacao         3591 non-null   float64       \n",
      " 7   Target           3591 non-null   int64         \n",
      " 8   MA_5             3587 non-null   float64       \n",
      " 9   MA_10            3582 non-null   float64       \n",
      " 10  MA_20            3572 non-null   float64       \n",
      " 11  MA_50            3542 non-null   float64       \n",
      " 12  BB_Upper         3572 non-null   float64       \n",
      " 13  BB_Lower         3572 non-null   float64       \n",
      " 14  BB_Width         3572 non-null   float64       \n",
      " 15  BB_Position      3572 non-null   float64       \n",
      " 16  RSI              3578 non-null   float64       \n",
      " 17  MACD             3591 non-null   float64       \n",
      " 18  Signal_Line      3591 non-null   float64       \n",
      " 19  high_close_prev  3590 non-null   float64       \n",
      " 20  low_close_prev   3590 non-null   float64       \n",
      " 21  true_range       3591 non-null   float64       \n",
      " 22  atr_5            3587 non-null   float64       \n",
      " 23  atr_10           3582 non-null   float64       \n",
      " 24  atr_20           3572 non-null   float64       \n",
      " 25  returns          3590 non-null   float64       \n",
      " 26  volatility_5     3586 non-null   float64       \n",
      " 27  volatility_10    3581 non-null   float64       \n",
      " 28  volatility_20    3571 non-null   float64       \n",
      " 29  Price_Range      3591 non-null   float64       \n",
      " 30  Price_Position   3591 non-null   float64       \n",
      " 31  Gap              3590 non-null   float64       \n",
      " 32  hl_close_ratio   3591 non-null   float64       \n",
      " 33  day_of_week      3591 non-null   int32         \n",
      " 34  month            3591 non-null   int32         \n",
      " 35  quarter          3591 non-null   int32         \n",
      " 36  year             3591 non-null   int32         \n",
      " 37  is_month_start   3591 non-null   int64         \n",
      " 38  is_month_end     3591 non-null   int64         \n",
      " 39  is_quarter_end   3591 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(31), int32(4), int64(4)\n",
      "memory usage: 1.0 MB\n",
      "\n",
      "📂 CATEGORIZAÇÃO DE FEATURES:\n",
      "   🏢 OHLC/Volume (5): ['Último', 'Abertura', 'Máxima', 'Mínima', 'Volume']\n",
      "   📅 Temporais (8): ['Data', 'day_of_week', 'month', 'quarter', 'year', 'is_month_start', 'is_month_end', 'is_quarter_end']\n",
      "   📈 Técnicas (16): ['MA_5', 'MA_10', 'MA_20', 'MA_50', 'BB_Upper']...\n",
      "   ⏰ Shifted (0): []\n",
      "   🕯️ Candlestick (2): ['high_close_prev', 'low_close_prev']\n",
      "   🎯 Target (1): ['Target']\n",
      "   ❓ Outras (8): ['Variacao', 'Signal_Line', 'true_range', 'returns', 'Price_Range']...\n",
      "\n",
      "🚨 ANÁLISE DE MUDANÇAS:\n",
      "   📉 Features PERDIDAS (4):\n",
      "      ❌ BB_Middle\n",
      "      ❌ Var%\n",
      "      ❌ Vol.\n",
      "      ❌ high_low\n",
      "   📈 Features GANHAS (0):\n",
      "      ➖ Nenhuma feature ganha\n",
      "   📊 Mudança líquida: -4 features\n",
      "\n",
      "🔍 QUALIDADE DOS DADOS:\n",
      "   Colunas com valores ausentes: 19\n",
      "   Top 5 com mais NaNs:\n",
      "      MA_5: 4 (0.1%)\n",
      "      MA_10: 9 (0.3%)\n",
      "      MA_20: 19 (0.5%)\n",
      "      MA_50: 49 (1.4%)\n",
      "      BB_Upper: 19 (0.5%)\n",
      "\n",
      "✅ Auditoria de STEP 04 - Seleção de Features concluída\n",
      "============================================================\n",
      "\n",
      "📋 RESUMO STEP 04:\n",
      "   ✅ Features selecionadas: 40\n",
      "   📊 Features removidas: 4\n",
      "   🎯 Shape final: (3591, 40)\n"
     ]
    }
   ],
   "source": [
    "# STEP 04: SELEÇÃO DE FEATURES - PONTO CRÍTICO!\n",
    "print(\"\\n📂 STEP 04: Seleção de features (PONTO CRÍTICO)...\")\n",
    "print(\"🚨 ATENÇÃO: Esta é uma área comum para 'PENHASCOS' de features!\")\n",
    "\n",
    "# ANTES da seleção - checkpoint crítico\n",
    "print(f\"\\n🔍 CHECKPOINT ANTES DA SELEÇÃO:\")\n",
    "print(f\"   Features disponíveis: {df_step03_features.shape[1]}\")\n",
    "print(f\"   Primeiras 10: {list(df_step03_features.columns)[:10]}\")\n",
    "print(f\"   Últimas 10: {list(df_step03_features.columns)[-10:]}\")\n",
    "\n",
    "# Criar cópia para seleção\n",
    "df_step04_selected = df_step03_features.copy()\n",
    "\n",
    "# ESTRATÉGIA CONSERVADORA: Manter TODAS as features criadas\n",
    "# Apenas remover colunas auxiliares e duplicadas\n",
    "columns_to_remove = [\n",
    "    'Vol.',  # Versão string do volume\n",
    "    'Var%',  # Versão string da variação\n",
    "    'high_low',  # Auxiliar para ATR\n",
    "    'BB_Middle'  # Duplicata da MA_20\n",
    "]\n",
    "\n",
    "# Remover apenas colunas auxiliares\n",
    "columns_to_remove_existing = [col for col in columns_to_remove if col in df_step04_selected.columns]\n",
    "if columns_to_remove_existing:\n",
    "    print(f\"\\n🗑️ Removendo apenas colunas auxiliares: {columns_to_remove_existing}\")\n",
    "    df_step04_selected = df_step04_selected.drop(columns=columns_to_remove_existing)\n",
    "else:\n",
    "    print(f\"\\n✅ Nenhuma coluna auxiliar para remover\")\n",
    "\n",
    "# CHECKPOINT CRÍTICO: Verificar se não perdemos features importantes\n",
    "features_importantes = [\n",
    "    'Abertura', 'Máxima', 'Mínima', 'Último', 'Volume',\n",
    "    'MA_5', 'MA_10', 'MA_20', 'MA_50',\n",
    "    'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position',\n",
    "    'RSI', 'MACD', 'Signal_Line',\n",
    "    'atr_5', 'atr_10', 'atr_20',\n",
    "    'volatility_5', 'volatility_10', 'volatility_20',\n",
    "    'Price_Range', 'Price_Position', 'Gap', 'hl_close_ratio',\n",
    "    'day_of_week', 'month', 'quarter', 'year',\n",
    "    'is_month_start', 'is_month_end', 'is_quarter_end',\n",
    "    'Target'\n",
    "]\n",
    "\n",
    "features_perdidas = [f for f in features_importantes if f not in df_step04_selected.columns]\n",
    "if features_perdidas:\n",
    "    print(f\"\\n🚨 ALERTA: Features importantes perdidas: {features_perdidas}\")\n",
    "else:\n",
    "    print(f\"\\n✅ Todas as features importantes preservadas\")\n",
    "\n",
    "# Auditoria da seleção\n",
    "audit_step04 = audit_pipeline_step(\n",
    "    df_step04_selected, \n",
    "    \"STEP 04 - Seleção de Features\", \n",
    "    expected_min_cols=len(features_importantes),\n",
    "    previous_df=df_step03_features\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 RESUMO STEP 04:\")\n",
    "print(f\"   ✅ Features selecionadas: {df_step04_selected.shape[1]}\")\n",
    "print(f\"   📊 Features removidas: {df_step03_features.shape[1] - df_step04_selected.shape[1]}\")\n",
    "print(f\"   🎯 Shape final: {df_step04_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 05: CORREÇÃO TEMPORAL (PENHASCO CRÍTICO!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 STEP 05: Correção temporal (PENHASCO CRÍTICO!)...\n",
      "🚨 ATENÇÃO: Esta é a área mais provável para perda massiva de features!\n",
      "\n",
      "🔍 CHECKPOINT ANTES DA CORREÇÃO TEMPORAL:\n",
      "   Features disponíveis: 40\n",
      "   Shape: (3591, 40)\n",
      "\n",
      "📊 ANÁLISE DE FEATURES PARA SHIFT:\n",
      "   Features para shift: 31 de 31\n",
      "   Features sem shift: 9 de 9\n",
      "\n",
      "⏰ Aplicando shift temporal...\n",
      "\n",
      "🔍 CHECKPOINT APÓS CORREÇÃO TEMPORAL:\n",
      "   Features antes: 40\n",
      "   Features depois: 40\n",
      "   Mudança: +0\n",
      "\n",
      "📊 VERIFICAÇÃO DE FEATURES SHIFTED:\n",
      "   Esperadas: 31\n",
      "   Criadas: 31\n",
      "   ✅ Todas as features shifted criadas corretamente\n",
      "\n",
      "============================================================\n",
      "🔍 AUDITORIA: STEP 05 - CORREÇÃO TEMPORAL\n",
      "============================================================\n",
      "📊 INFORMAÇÕES BÁSICAS:\n",
      "   Shape: (3590, 40)\n",
      "   Memória: 1.04 MB\n",
      "\n",
      "📋 DETALHES DAS COLUNAS (.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3590 entries, 0 to 3589\n",
      "Data columns (total 40 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   Data                     3590 non-null   datetime64[ns]\n",
      " 1   day_of_week              3590 non-null   int32         \n",
      " 2   month                    3590 non-null   int32         \n",
      " 3   quarter                  3590 non-null   int32         \n",
      " 4   year                     3590 non-null   int32         \n",
      " 5   is_month_start           3590 non-null   int64         \n",
      " 6   is_month_end             3590 non-null   int64         \n",
      " 7   is_quarter_end           3590 non-null   int64         \n",
      " 8   Target                   3590 non-null   int64         \n",
      " 9   Abertura_shifted         3590 non-null   float64       \n",
      " 10  Máxima_shifted           3590 non-null   float64       \n",
      " 11  Mínima_shifted           3590 non-null   float64       \n",
      " 12  Último_shifted           3590 non-null   float64       \n",
      " 13  Volume_shifted           3590 non-null   float64       \n",
      " 14  MA_5_shifted             3586 non-null   float64       \n",
      " 15  MA_10_shifted            3581 non-null   float64       \n",
      " 16  MA_20_shifted            3571 non-null   float64       \n",
      " 17  MA_50_shifted            3541 non-null   float64       \n",
      " 18  BB_Upper_shifted         3571 non-null   float64       \n",
      " 19  BB_Lower_shifted         3571 non-null   float64       \n",
      " 20  BB_Width_shifted         3571 non-null   float64       \n",
      " 21  BB_Position_shifted      3571 non-null   float64       \n",
      " 22  RSI_shifted              3577 non-null   float64       \n",
      " 23  MACD_shifted             3590 non-null   float64       \n",
      " 24  Signal_Line_shifted      3590 non-null   float64       \n",
      " 25  atr_5_shifted            3586 non-null   float64       \n",
      " 26  atr_10_shifted           3581 non-null   float64       \n",
      " 27  atr_20_shifted           3571 non-null   float64       \n",
      " 28  volatility_5_shifted     3585 non-null   float64       \n",
      " 29  volatility_10_shifted    3580 non-null   float64       \n",
      " 30  volatility_20_shifted    3570 non-null   float64       \n",
      " 31  Price_Range_shifted      3590 non-null   float64       \n",
      " 32  Price_Position_shifted   3590 non-null   float64       \n",
      " 33  Gap_shifted              3589 non-null   float64       \n",
      " 34  hl_close_ratio_shifted   3590 non-null   float64       \n",
      " 35  true_range_shifted       3590 non-null   float64       \n",
      " 36  high_close_prev_shifted  3589 non-null   float64       \n",
      " 37  low_close_prev_shifted   3589 non-null   float64       \n",
      " 38  returns_shifted          3589 non-null   float64       \n",
      " 39  Variacao_shifted         3590 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(31), int32(4), int64(4)\n",
      "memory usage: 1.0 MB\n",
      "\n",
      "📂 CATEGORIZAÇÃO DE FEATURES:\n",
      "   🏢 OHLC/Volume (0): []\n",
      "   📅 Temporais (8): ['Data', 'day_of_week', 'month', 'quarter', 'year', 'is_month_start', 'is_month_end', 'is_quarter_end']\n",
      "   📈 Técnicas (18): ['Máxima_shifted', 'Mínima_shifted', 'MA_5_shifted', 'MA_10_shifted', 'MA_20_shifted']...\n",
      "   ⏰ Shifted (31): ['Abertura_shifted', 'Máxima_shifted', 'Mínima_shifted', 'Último_shifted', 'Volume_shifted']...\n",
      "   🕯️ Candlestick (2): ['high_close_prev_shifted', 'low_close_prev_shifted']\n",
      "   🎯 Target (1): ['Target']\n",
      "   ❓ Outras (0): []\n",
      "\n",
      "🚨 ANÁLISE DE MUDANÇAS:\n",
      "   📉 Features PERDIDAS (31):\n",
      "      ❌ Abertura\n",
      "      ❌ BB_Lower\n",
      "      ❌ BB_Position\n",
      "      ❌ BB_Upper\n",
      "      ❌ BB_Width\n",
      "      ❌ Gap\n",
      "      ❌ MACD\n",
      "      ❌ MA_10\n",
      "      ❌ MA_20\n",
      "      ❌ MA_5\n",
      "      ❌ MA_50\n",
      "      ❌ Máxima\n",
      "      ❌ Mínima\n",
      "      ❌ Price_Position\n",
      "      ❌ Price_Range\n",
      "      ❌ RSI\n",
      "      ❌ Signal_Line\n",
      "      ❌ Variacao\n",
      "      ❌ Volume\n",
      "      ❌ atr_10\n",
      "      ❌ atr_20\n",
      "      ❌ atr_5\n",
      "      ❌ high_close_prev\n",
      "      ❌ hl_close_ratio\n",
      "      ❌ low_close_prev\n",
      "      ❌ returns\n",
      "      ❌ true_range\n",
      "      ❌ volatility_10\n",
      "      ❌ volatility_20\n",
      "      ❌ volatility_5\n",
      "      ❌ Último\n",
      "   📈 Features GANHAS (31):\n",
      "      ✅ Abertura_shifted\n",
      "      ✅ BB_Lower_shifted\n",
      "      ✅ BB_Position_shifted\n",
      "      ✅ BB_Upper_shifted\n",
      "      ✅ BB_Width_shifted\n",
      "      ✅ Gap_shifted\n",
      "      ✅ MACD_shifted\n",
      "      ✅ MA_10_shifted\n",
      "      ✅ MA_20_shifted\n",
      "      ✅ MA_50_shifted\n",
      "      ✅ MA_5_shifted\n",
      "      ✅ Máxima_shifted\n",
      "      ✅ Mínima_shifted\n",
      "      ✅ Price_Position_shifted\n",
      "      ✅ Price_Range_shifted\n",
      "      ✅ RSI_shifted\n",
      "      ✅ Signal_Line_shifted\n",
      "      ✅ Variacao_shifted\n",
      "      ✅ Volume_shifted\n",
      "      ✅ atr_10_shifted\n",
      "      ✅ atr_20_shifted\n",
      "      ✅ atr_5_shifted\n",
      "      ✅ high_close_prev_shifted\n",
      "      ✅ hl_close_ratio_shifted\n",
      "      ✅ low_close_prev_shifted\n",
      "      ✅ returns_shifted\n",
      "      ✅ true_range_shifted\n",
      "      ✅ volatility_10_shifted\n",
      "      ✅ volatility_20_shifted\n",
      "      ✅ volatility_5_shifted\n",
      "      ✅ Último_shifted\n",
      "   📊 Mudança líquida: +0 features\n",
      "\n",
      "🚨 ALERTA: PERDA SIGNIFICATIVA DE FEATURES!\n",
      "   31 features foram perdidas nesta etapa\n",
      "   Isso pode indicar um 'PENHASCO' no pipeline\n",
      "\n",
      "🔍 QUALIDADE DOS DADOS:\n",
      "   Colunas com valores ausentes: 19\n",
      "   Top 5 com mais NaNs:\n",
      "      MA_5_shifted: 4 (0.1%)\n",
      "      MA_10_shifted: 9 (0.3%)\n",
      "      MA_20_shifted: 19 (0.5%)\n",
      "      MA_50_shifted: 49 (1.4%)\n",
      "      BB_Upper_shifted: 19 (0.5%)\n",
      "\n",
      "✅ Auditoria de STEP 05 - Correção Temporal concluída\n",
      "============================================================\n",
      "\n",
      "📋 RESUMO STEP 05:\n",
      "   ✅ Correção temporal aplicada\n",
      "   📊 Features shifted criadas: 31\n",
      "   🎯 Shape final: (3590, 40)\n"
     ]
    }
   ],
   "source": [
    "# STEP 05: CORREÇÃO TEMPORAL - PENHASCO CRÍTICO!\n",
    "print(\"\\n📂 STEP 05: Correção temporal (PENHASCO CRÍTICO!)...\")\n",
    "print(\"🚨 ATENÇÃO: Esta é a área mais provável para perda massiva de features!\")\n",
    "\n",
    "# CHECKPOINT ANTES DA CORREÇÃO TEMPORAL\n",
    "print(f\"\\n🔍 CHECKPOINT ANTES DA CORREÇÃO TEMPORAL:\")\n",
    "print(f\"   Features disponíveis: {df_step04_selected.shape[1]}\")\n",
    "print(f\"   Shape: {df_step04_selected.shape}\")\n",
    "\n",
    "# Criar cópia para correção temporal\n",
    "df_step05_temporal = df_step04_selected.copy()\n",
    "\n",
    "# ESTRATÉGIA CONSERVADORA: Aplicar shift apenas onde necessário\n",
    "# Identificar features que precisam de shift (dados do presente/futuro)\n",
    "features_to_shift = [\n",
    "    'Abertura', 'Máxima', 'Mínima', 'Último', 'Volume',\n",
    "    'MA_5', 'MA_10', 'MA_20', 'MA_50',\n",
    "    'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Position',\n",
    "    'RSI', 'MACD', 'Signal_Line',\n",
    "    'atr_5', 'atr_10', 'atr_20',\n",
    "    'volatility_5', 'volatility_10', 'volatility_20',\n",
    "    'Price_Range', 'Price_Position', 'Gap', 'hl_close_ratio',\n",
    "    'true_range', 'high_close_prev', 'low_close_prev',\n",
    "    'returns', 'Variacao'\n",
    "]\n",
    "\n",
    "# Features que NÃO precisam de shift (dados temporais do passado)\n",
    "features_no_shift = [\n",
    "    'Data', 'day_of_week', 'month', 'quarter', 'year',\n",
    "    'is_month_start', 'is_month_end', 'is_quarter_end',\n",
    "    'Target'\n",
    "]\n",
    "\n",
    "# Verificar quais features estão disponíveis\n",
    "available_to_shift = [f for f in features_to_shift if f in df_step05_temporal.columns]\n",
    "available_no_shift = [f for f in features_no_shift if f in df_step05_temporal.columns]\n",
    "\n",
    "print(f\"\\n📊 ANÁLISE DE FEATURES PARA SHIFT:\")\n",
    "print(f\"   Features para shift: {len(available_to_shift)} de {len(features_to_shift)}\")\n",
    "print(f\"   Features sem shift: {len(available_no_shift)} de {len(features_no_shift)}\")\n",
    "\n",
    "missing_to_shift = [f for f in features_to_shift if f not in df_step05_temporal.columns]\n",
    "if missing_to_shift:\n",
    "    print(f\"   ⚠️ Features ausentes para shift: {missing_to_shift}\")\n",
    "\n",
    "# APLICAR SHIFT DE FORMA CONSERVADORA\n",
    "print(f\"\\n⏰ Aplicando shift temporal...\")\n",
    "\n",
    "# Criar dataset final com features shifted\n",
    "df_temporal_final = pd.DataFrame()\n",
    "\n",
    "# 1. Manter features temporais sem shift\n",
    "for feature in available_no_shift:\n",
    "    df_temporal_final[feature] = df_step05_temporal[feature]\n",
    "\n",
    "# 2. Aplicar shift nas features de preço/indicadores\n",
    "for feature in available_to_shift:\n",
    "    df_temporal_final[f'{feature}_shifted'] = df_step05_temporal[feature].shift(1)\n",
    "\n",
    "# 3. Remover primeira linha (NaN devido ao shift)\n",
    "df_temporal_final = df_temporal_final.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "# CHECKPOINT CRÍTICO APÓS CORREÇÃO TEMPORAL\n",
    "print(f\"\\n🔍 CHECKPOINT APÓS CORREÇÃO TEMPORAL:\")\n",
    "print(f\"   Features antes: {df_step05_temporal.shape[1]}\")\n",
    "print(f\"   Features depois: {df_temporal_final.shape[1]}\")\n",
    "print(f\"   Mudança: {df_temporal_final.shape[1] - df_step05_temporal.shape[1]:+d}\")\n",
    "\n",
    "# Verificar se temos as features shifted esperadas\n",
    "expected_shifted = [f'{f}_shifted' for f in available_to_shift]\n",
    "actual_shifted = [col for col in df_temporal_final.columns if col.endswith('_shifted')]\n",
    "\n",
    "print(f\"\\n📊 VERIFICAÇÃO DE FEATURES SHIFTED:\")\n",
    "print(f\"   Esperadas: {len(expected_shifted)}\")\n",
    "print(f\"   Criadas: {len(actual_shifted)}\")\n",
    "\n",
    "missing_shifted = [f for f in expected_shifted if f not in df_temporal_final.columns]\n",
    "if missing_shifted:\n",
    "    print(f\"   ❌ Features shifted ausentes: {missing_shifted[:5]}{'...' if len(missing_shifted) > 5 else ''}\")\n",
    "else:\n",
    "    print(f\"   ✅ Todas as features shifted criadas corretamente\")\n",
    "\n",
    "# Atualizar variável para próxima etapa\n",
    "df_step05_temporal = df_temporal_final.copy()\n",
    "\n",
    "# Auditoria da correção temporal\n",
    "audit_step05 = audit_pipeline_step(\n",
    "    df_step05_temporal, \n",
    "    \"STEP 05 - Correção Temporal\", \n",
    "    expected_min_cols=len(available_no_shift) + len(available_to_shift),\n",
    "    previous_df=df_step04_selected\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 RESUMO STEP 05:\")\n",
    "print(f\"   ✅ Correção temporal aplicada\")\n",
    "print(f\"   📊 Features shifted criadas: {len(actual_shifted)}\")\n",
    "print(f\"   🎯 Shape final: {df_step05_temporal.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 06: PADRÕES DE CANDLESTICK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 STEP 06: Criando padrões de candlestick...\n",
      "\n",
      "🕯️ VERIFICAÇÃO DE COLUNAS OHLC SHIFTED:\n",
      "   Necessárias: ['Abertura_shifted', 'Máxima_shifted', 'Mínima_shifted', 'Último_shifted']\n",
      "   Disponíveis: ['Abertura_shifted', 'Máxima_shifted', 'Mínima_shifted', 'Último_shifted']\n",
      "   ✅ Todas as colunas OHLC shifted disponíveis\n",
      "\n",
      "📊 Criando padrões de candlestick...\n",
      "   ✅ 9 features de candlestick criadas\n",
      "\n",
      "============================================================\n",
      "🔍 AUDITORIA: STEP 06 - PADRÕES DE CANDLESTICK\n",
      "============================================================\n",
      "📊 INFORMAÇÕES BÁSICAS:\n",
      "   Shape: (3590, 49)\n",
      "   Memória: 1.29 MB\n",
      "\n",
      "📋 DETALHES DAS COLUNAS (.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3590 entries, 0 to 3589\n",
      "Data columns (total 49 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   Data                     3590 non-null   datetime64[ns]\n",
      " 1   day_of_week              3590 non-null   int32         \n",
      " 2   month                    3590 non-null   int32         \n",
      " 3   quarter                  3590 non-null   int32         \n",
      " 4   year                     3590 non-null   int32         \n",
      " 5   is_month_start           3590 non-null   int64         \n",
      " 6   is_month_end             3590 non-null   int64         \n",
      " 7   is_quarter_end           3590 non-null   int64         \n",
      " 8   Target                   3590 non-null   int64         \n",
      " 9   Abertura_shifted         3590 non-null   float64       \n",
      " 10  Máxima_shifted           3590 non-null   float64       \n",
      " 11  Mínima_shifted           3590 non-null   float64       \n",
      " 12  Último_shifted           3590 non-null   float64       \n",
      " 13  Volume_shifted           3590 non-null   float64       \n",
      " 14  MA_5_shifted             3586 non-null   float64       \n",
      " 15  MA_10_shifted            3581 non-null   float64       \n",
      " 16  MA_20_shifted            3571 non-null   float64       \n",
      " 17  MA_50_shifted            3541 non-null   float64       \n",
      " 18  BB_Upper_shifted         3571 non-null   float64       \n",
      " 19  BB_Lower_shifted         3571 non-null   float64       \n",
      " 20  BB_Width_shifted         3571 non-null   float64       \n",
      " 21  BB_Position_shifted      3571 non-null   float64       \n",
      " 22  RSI_shifted              3577 non-null   float64       \n",
      " 23  MACD_shifted             3590 non-null   float64       \n",
      " 24  Signal_Line_shifted      3590 non-null   float64       \n",
      " 25  atr_5_shifted            3586 non-null   float64       \n",
      " 26  atr_10_shifted           3581 non-null   float64       \n",
      " 27  atr_20_shifted           3571 non-null   float64       \n",
      " 28  volatility_5_shifted     3585 non-null   float64       \n",
      " 29  volatility_10_shifted    3580 non-null   float64       \n",
      " 30  volatility_20_shifted    3570 non-null   float64       \n",
      " 31  Price_Range_shifted      3590 non-null   float64       \n",
      " 32  Price_Position_shifted   3590 non-null   float64       \n",
      " 33  Gap_shifted              3589 non-null   float64       \n",
      " 34  hl_close_ratio_shifted   3590 non-null   float64       \n",
      " 35  true_range_shifted       3590 non-null   float64       \n",
      " 36  high_close_prev_shifted  3589 non-null   float64       \n",
      " 37  low_close_prev_shifted   3589 non-null   float64       \n",
      " 38  returns_shifted          3589 non-null   float64       \n",
      " 39  Variacao_shifted         3590 non-null   float64       \n",
      " 40  doji_prev                3590 non-null   int64         \n",
      " 41  hammer_prev              3590 non-null   int64         \n",
      " 42  shooting_star_prev       3590 non-null   int64         \n",
      " 43  engulfing_bullish_prev   3590 non-null   int64         \n",
      " 44  bullish_candle_prev      3590 non-null   int64         \n",
      " 45  bearish_candle_prev      3590 non-null   int64         \n",
      " 46  body_size_prev           3590 non-null   float64       \n",
      " 47  upper_shadow_prev        3590 non-null   float64       \n",
      " 48  lower_shadow_prev        3590 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(34), int32(4), int64(10)\n",
      "memory usage: 1.3 MB\n",
      "\n",
      "📂 CATEGORIZAÇÃO DE FEATURES:\n",
      "   🏢 OHLC/Volume (0): []\n",
      "   📅 Temporais (8): ['Data', 'day_of_week', 'month', 'quarter', 'year', 'is_month_start', 'is_month_end', 'is_quarter_end']\n",
      "   📈 Técnicas (18): ['Máxima_shifted', 'Mínima_shifted', 'MA_5_shifted', 'MA_10_shifted', 'MA_20_shifted']...\n",
      "   ⏰ Shifted (31): ['Abertura_shifted', 'Máxima_shifted', 'Mínima_shifted', 'Último_shifted', 'Volume_shifted']...\n",
      "   🕯️ Candlestick (11): ['high_close_prev_shifted', 'low_close_prev_shifted', 'doji_prev', 'hammer_prev', 'shooting_star_prev']...\n",
      "   🎯 Target (1): ['Target']\n",
      "   ❓ Outras (0): []\n",
      "\n",
      "🚨 ANÁLISE DE MUDANÇAS:\n",
      "   📉 Features PERDIDAS (0):\n",
      "      ✅ Nenhuma feature perdida\n",
      "   📈 Features GANHAS (9):\n",
      "      ✅ bearish_candle_prev\n",
      "      ✅ body_size_prev\n",
      "      ✅ bullish_candle_prev\n",
      "      ✅ doji_prev\n",
      "      ✅ engulfing_bullish_prev\n",
      "      ✅ hammer_prev\n",
      "      ✅ lower_shadow_prev\n",
      "      ✅ shooting_star_prev\n",
      "      ✅ upper_shadow_prev\n",
      "   📊 Mudança líquida: +9 features\n",
      "\n",
      "🔍 QUALIDADE DOS DADOS:\n",
      "   Colunas com valores ausentes: 19\n",
      "   Top 5 com mais NaNs:\n",
      "      MA_5_shifted: 4 (0.1%)\n",
      "      MA_10_shifted: 9 (0.3%)\n",
      "      MA_20_shifted: 19 (0.5%)\n",
      "      MA_50_shifted: 49 (1.4%)\n",
      "      BB_Upper_shifted: 19 (0.5%)\n",
      "\n",
      "✅ Auditoria de STEP 06 - Padrões de Candlestick concluída\n",
      "============================================================\n",
      "\n",
      "📋 RESUMO STEP 06:\n",
      "   ✅ Features de candlestick criadas: 9\n",
      "   📊 Total de features: 49\n",
      "   🎯 Shape final: (3590, 49)\n"
     ]
    }
   ],
   "source": [
    "# STEP 06: PADRÕES DE CANDLESTICK\n",
    "print(\"\\n📂 STEP 06: Criando padrões de candlestick...\")\n",
    "\n",
    "# Criar cópia para candlestick\n",
    "df_step06_candlestick = df_step05_temporal.copy()\n",
    "\n",
    "# Verificar se temos as colunas OHLC shifted necessárias\n",
    "ohlc_shifted_cols = ['Abertura_shifted', 'Máxima_shifted', 'Mínima_shifted', 'Último_shifted']\n",
    "available_ohlc_shifted = [col for col in ohlc_shifted_cols if col in df_step06_candlestick.columns]\n",
    "\n",
    "print(f\"\\n🕯️ VERIFICAÇÃO DE COLUNAS OHLC SHIFTED:\")\n",
    "print(f\"   Necessárias: {ohlc_shifted_cols}\")\n",
    "print(f\"   Disponíveis: {available_ohlc_shifted}\")\n",
    "\n",
    "if len(available_ohlc_shifted) == 4:\n",
    "    print(f\"   ✅ Todas as colunas OHLC shifted disponíveis\")\n",
    "    \n",
    "    # Renomear temporariamente para facilitar cálculos\n",
    "    ohlc_temp = df_step06_candlestick[available_ohlc_shifted].copy()\n",
    "    ohlc_temp.columns = ['Open', 'High', 'Low', 'Close']\n",
    "    \n",
    "    # Funções de padrões de candlestick\n",
    "    def detect_doji(df, threshold=0.1):\n",
    "        body_size = abs(df['Close'] - df['Open'])\n",
    "        total_range = df['High'] - df['Low']\n",
    "        return (body_size / total_range < threshold).astype(int)\n",
    "    \n",
    "    def detect_hammer(df):\n",
    "        body_size = abs(df['Close'] - df['Open'])\n",
    "        lower_shadow = df[['Open', 'Close']].min(axis=1) - df['Low']\n",
    "        upper_shadow = df['High'] - df[['Open', 'Close']].max(axis=1)\n",
    "        return ((lower_shadow > 2 * body_size) & (upper_shadow < body_size)).astype(int)\n",
    "    \n",
    "    def detect_shooting_star(df):\n",
    "        body_size = abs(df['Close'] - df['Open'])\n",
    "        lower_shadow = df[['Open', 'Close']].min(axis=1) - df['Low']\n",
    "        upper_shadow = df['High'] - df[['Open', 'Close']].max(axis=1)\n",
    "        return ((upper_shadow > 2 * body_size) & (lower_shadow < body_size)).astype(int)\n",
    "    \n",
    "    def detect_engulfing_bullish(df):\n",
    "        current_bullish = df['Close'] > df['Open']\n",
    "        prev_bearish = (df['Close'].shift(1) < df['Open'].shift(1))\n",
    "        current_open_below_prev_close = df['Open'] < df['Close'].shift(1)\n",
    "        current_close_above_prev_open = df['Close'] > df['Open'].shift(1)\n",
    "        return (current_bullish & prev_bearish & current_open_below_prev_close & current_close_above_prev_open).astype(int)\n",
    "    \n",
    "    # Aplicar padrões\n",
    "    print(f\"\\n📊 Criando padrões de candlestick...\")\n",
    "    \n",
    "    df_step06_candlestick['doji_prev'] = detect_doji(ohlc_temp)\n",
    "    df_step06_candlestick['hammer_prev'] = detect_hammer(ohlc_temp)\n",
    "    df_step06_candlestick['shooting_star_prev'] = detect_shooting_star(ohlc_temp)\n",
    "    df_step06_candlestick['engulfing_bullish_prev'] = detect_engulfing_bullish(ohlc_temp)\n",
    "    \n",
    "    # Padrões adicionais\n",
    "    df_step06_candlestick['bullish_candle_prev'] = (ohlc_temp['Close'] > ohlc_temp['Open']).astype(int)\n",
    "    df_step06_candlestick['bearish_candle_prev'] = (ohlc_temp['Close'] < ohlc_temp['Open']).astype(int)\n",
    "    \n",
    "    # Métricas de candlestick\n",
    "    df_step06_candlestick['body_size_prev'] = abs(ohlc_temp['Close'] - ohlc_temp['Open'])\n",
    "    df_step06_candlestick['upper_shadow_prev'] = ohlc_temp['High'] - ohlc_temp[['Open', 'Close']].max(axis=1)\n",
    "    df_step06_candlestick['lower_shadow_prev'] = ohlc_temp[['Open', 'Close']].min(axis=1) - ohlc_temp['Low']\n",
    "    \n",
    "    candlestick_features_created = 9\n",
    "    print(f\"   ✅ {candlestick_features_created} features de candlestick criadas\")\n",
    "    \n",
    "else:\n",
    "    print(f\"   ❌ Colunas OHLC shifted insuficientes\")\n",
    "    print(f\"   Ausentes: {[col for col in ohlc_shifted_cols if col not in df_step06_candlestick.columns]}\")\n",
    "    candlestick_features_created = 0\n",
    "\n",
    "# Auditoria dos padrões de candlestick\n",
    "audit_step06 = audit_pipeline_step(\n",
    "    df_step06_candlestick, \n",
    "    \"STEP 06 - Padrões de Candlestick\", \n",
    "    expected_min_cols=df_step05_temporal.shape[1] + candlestick_features_created,\n",
    "    previous_df=df_step05_temporal\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 RESUMO STEP 06:\")\n",
    "print(f\"   ✅ Features de candlestick criadas: {candlestick_features_created}\")\n",
    "print(f\"   📊 Total de features: {df_step06_candlestick.shape[1]}\")\n",
    "print(f\"   🎯 Shape final: {df_step06_candlestick.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 07: DATASET FINAL E AUDITORIA COMPLETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📂 STEP 07: Preparando dataset final...\n",
      "\n",
      "🧹 Limpeza final de dados...\n",
      "   Shape antes da limpeza: (3590, 49)\n",
      "   Shape após limpeza: (3586, 49)\n",
      "   Linhas removidas: 4\n",
      "\n",
      "============================================================\n",
      "🔍 AUDITORIA: STEP 07 - DATASET FINAL\n",
      "============================================================\n",
      "📊 INFORMAÇÕES BÁSICAS:\n",
      "   Shape: (3586, 49)\n",
      "   Memória: 1.31 MB\n",
      "\n",
      "📋 DETALHES DAS COLUNAS (.info()):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3586 entries, 4 to 3589\n",
      "Data columns (total 49 columns):\n",
      " #   Column                   Non-Null Count  Dtype         \n",
      "---  ------                   --------------  -----         \n",
      " 0   Data                     3586 non-null   datetime64[ns]\n",
      " 1   day_of_week              3586 non-null   int32         \n",
      " 2   month                    3586 non-null   int32         \n",
      " 3   quarter                  3586 non-null   int32         \n",
      " 4   year                     3586 non-null   int32         \n",
      " 5   is_month_start           3586 non-null   int64         \n",
      " 6   is_month_end             3586 non-null   int64         \n",
      " 7   is_quarter_end           3586 non-null   int64         \n",
      " 8   Target                   3586 non-null   int64         \n",
      " 9   Abertura_shifted         3586 non-null   float64       \n",
      " 10  Máxima_shifted           3586 non-null   float64       \n",
      " 11  Mínima_shifted           3586 non-null   float64       \n",
      " 12  Último_shifted           3586 non-null   float64       \n",
      " 13  Volume_shifted           3586 non-null   float64       \n",
      " 14  MA_5_shifted             3586 non-null   float64       \n",
      " 15  MA_10_shifted            3581 non-null   float64       \n",
      " 16  MA_20_shifted            3571 non-null   float64       \n",
      " 17  MA_50_shifted            3541 non-null   float64       \n",
      " 18  BB_Upper_shifted         3571 non-null   float64       \n",
      " 19  BB_Lower_shifted         3571 non-null   float64       \n",
      " 20  BB_Width_shifted         3571 non-null   float64       \n",
      " 21  BB_Position_shifted      3571 non-null   float64       \n",
      " 22  RSI_shifted              3577 non-null   float64       \n",
      " 23  MACD_shifted             3586 non-null   float64       \n",
      " 24  Signal_Line_shifted      3586 non-null   float64       \n",
      " 25  atr_5_shifted            3586 non-null   float64       \n",
      " 26  atr_10_shifted           3581 non-null   float64       \n",
      " 27  atr_20_shifted           3571 non-null   float64       \n",
      " 28  volatility_5_shifted     3585 non-null   float64       \n",
      " 29  volatility_10_shifted    3580 non-null   float64       \n",
      " 30  volatility_20_shifted    3570 non-null   float64       \n",
      " 31  Price_Range_shifted      3586 non-null   float64       \n",
      " 32  Price_Position_shifted   3586 non-null   float64       \n",
      " 33  Gap_shifted              3586 non-null   float64       \n",
      " 34  hl_close_ratio_shifted   3586 non-null   float64       \n",
      " 35  true_range_shifted       3586 non-null   float64       \n",
      " 36  high_close_prev_shifted  3586 non-null   float64       \n",
      " 37  low_close_prev_shifted   3586 non-null   float64       \n",
      " 38  returns_shifted          3586 non-null   float64       \n",
      " 39  Variacao_shifted         3586 non-null   float64       \n",
      " 40  doji_prev                3586 non-null   int64         \n",
      " 41  hammer_prev              3586 non-null   int64         \n",
      " 42  shooting_star_prev       3586 non-null   int64         \n",
      " 43  engulfing_bullish_prev   3586 non-null   int64         \n",
      " 44  bullish_candle_prev      3586 non-null   int64         \n",
      " 45  bearish_candle_prev      3586 non-null   int64         \n",
      " 46  body_size_prev           3586 non-null   float64       \n",
      " 47  upper_shadow_prev        3586 non-null   float64       \n",
      " 48  lower_shadow_prev        3586 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(34), int32(4), int64(10)\n",
      "memory usage: 1.3 MB\n",
      "\n",
      "📂 CATEGORIZAÇÃO DE FEATURES:\n",
      "   🏢 OHLC/Volume (0): []\n",
      "   📅 Temporais (8): ['Data', 'day_of_week', 'month', 'quarter', 'year', 'is_month_start', 'is_month_end', 'is_quarter_end']\n",
      "   📈 Técnicas (18): ['Máxima_shifted', 'Mínima_shifted', 'MA_5_shifted', 'MA_10_shifted', 'MA_20_shifted']...\n",
      "   ⏰ Shifted (31): ['Abertura_shifted', 'Máxima_shifted', 'Mínima_shifted', 'Último_shifted', 'Volume_shifted']...\n",
      "   🕯️ Candlestick (11): ['high_close_prev_shifted', 'low_close_prev_shifted', 'doji_prev', 'hammer_prev', 'shooting_star_prev']...\n",
      "   🎯 Target (1): ['Target']\n",
      "   ❓ Outras (0): []\n",
      "\n",
      "🚨 ANÁLISE DE MUDANÇAS:\n",
      "   📉 Features PERDIDAS (0):\n",
      "      ✅ Nenhuma feature perdida\n",
      "   📈 Features GANHAS (0):\n",
      "      ➖ Nenhuma feature ganha\n",
      "   📊 Mudança líquida: +0 features\n",
      "\n",
      "⚠️ AVISO: Menos colunas que esperado\n",
      "   Esperado: >= 50\n",
      "   Atual: 49\n",
      "\n",
      "🔍 QUALIDADE DOS DADOS:\n",
      "   Colunas com valores ausentes: 13\n",
      "   Top 5 com mais NaNs:\n",
      "      MA_10_shifted: 5 (0.1%)\n",
      "      MA_20_shifted: 15 (0.4%)\n",
      "      MA_50_shifted: 45 (1.3%)\n",
      "      BB_Upper_shifted: 15 (0.4%)\n",
      "      BB_Lower_shifted: 15 (0.4%)\n",
      "\n",
      "✅ Auditoria de STEP 07 - Dataset Final concluída\n",
      "============================================================\n",
      "\n",
      "📋 RESUMO STEP 07:\n",
      "   ✅ Dataset final preparado\n",
      "   📊 Features finais: 49\n",
      "   🎯 Shape final: (3586, 49)\n",
      "   💾 Pronto para modelagem!\n"
     ]
    }
   ],
   "source": [
    "# STEP 07: DATASET FINAL E AUDITORIA COMPLETA\n",
    "print(\"\\n📂 STEP 07: Preparando dataset final...\")\n",
    "\n",
    "# Criar dataset final\n",
    "df_step07_final = df_step06_candlestick.copy()\n",
    "\n",
    "# Limpeza final - remover linhas com muitos NaN\n",
    "print(f\"\\n🧹 Limpeza final de dados...\")\n",
    "print(f\"   Shape antes da limpeza: {df_step07_final.shape}\")\n",
    "\n",
    "# Contar NaN por linha\n",
    "nan_counts = df_step07_final.isnull().sum(axis=1)\n",
    "threshold = df_step07_final.shape[1] * 0.3  # Remover linhas com mais de 30% de NaN\n",
    "\n",
    "df_step07_final = df_step07_final[nan_counts <= threshold]\n",
    "print(f\"   Shape após limpeza: {df_step07_final.shape}\")\n",
    "print(f\"   Linhas removidas: {df_step06_candlestick.shape[0] - df_step07_final.shape[0]}\")\n",
    "\n",
    "# Auditoria final\n",
    "audit_step07 = audit_pipeline_step(\n",
    "    df_step07_final, \n",
    "    \"STEP 07 - Dataset Final\", \n",
    "    expected_min_cols=50,  # Esperamos pelo menos 50 features\n",
    "    previous_df=df_step06_candlestick\n",
    ")\n",
    "\n",
    "print(f\"\\n📋 RESUMO STEP 07:\")\n",
    "print(f\"   ✅ Dataset final preparado\")\n",
    "print(f\"   📊 Features finais: {df_step07_final.shape[1]}\")\n",
    "print(f\"   🎯 Shape final: {df_step07_final.shape}\")\n",
    "print(f\"   💾 Pronto para modelagem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUDITORIA COMPLETA DO PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🔍 AUDITORIA COMPLETA DO PIPELINE EXPLÍCITO\n",
      "================================================================================\n",
      "\n",
      "📊 RESUMO COMPLETO DO PIPELINE:\n",
      "Step                      Shape           Features   Técnicas   Shifted    Candlestick \n",
      "-------------------------------------------------------------------------------------\n",
      "STEP 01 - Dados Brutos    3592x7          7          0          0          0           \n",
      "STEP 02 - Processamento... 3591x10         10         0          0          0           \n",
      "STEP 03 - Features Técn... 3591x44         44         17         0          2           \n",
      "STEP 04 - Seleção de Fe... 3591x40         40         16         0          2           \n",
      "STEP 05 - Correção Temp... 3590x40         40         18         31         2           \n",
      "STEP 06 - Padrões de Ca... 3590x49         49         18         31         11          \n",
      "STEP 07 - Dataset Final   3586x49         49         18         31         11          \n",
      "\n",
      "📈 EVOLUÇÃO DAS FEATURES:\n",
      "   STEP 01 - Dados Brutos → STEP 02 - Processamento Básico: +3 features\n",
      "   STEP 02 - Processamento Básico → STEP 03 - Features Técnicas: +34 features\n",
      "   STEP 03 - Features Técnicas → STEP 04 - Seleção de Features: -4 features\n",
      "   STEP 04 - Seleção de Features → STEP 05 - Correção Temporal: 0 features\n",
      "   STEP 05 - Correção Temporal → STEP 06 - Padrões de Candlestick: +9 features\n",
      "   STEP 06 - Padrões de Candlestick → STEP 07 - Dataset Final: 0 features\n",
      "\n",
      "🚨 IDENTIFICAÇÃO DE PENHASCOS:\n",
      "   ✅ Nenhum penhasco significativo identificado\n",
      "\n",
      "✅ VERIFICAÇÃO FINAL:\n",
      "   Dataset inicial: (3592, 7)\n",
      "   Dataset final: (3586, 49)\n",
      "   Features criadas: +42\n",
      "   Linhas preservadas: 99.8%\n",
      "\n",
      "🎯 STATUS DO PIPELINE: ✅ BOM\n",
      "   Features finais: 49\n",
      "   Pronto para modelagem: SIM\n",
      "\n",
      "================================================================================\n",
      "🚀 PIPELINE EXPLÍCITO CONCLUÍDO COM SUCESSO!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# AUDITORIA COMPLETA DO PIPELINE\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🔍 AUDITORIA COMPLETA DO PIPELINE EXPLÍCITO\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Coletar todos os resultados de auditoria\n",
    "pipeline_steps = [\n",
    "    audit_step01, audit_step02, audit_step03, \n",
    "    audit_step04, audit_step05, audit_step06, audit_step07\n",
    "]\n",
    "\n",
    "# Tabela resumo\n",
    "print(f\"\\n📊 RESUMO COMPLETO DO PIPELINE:\")\n",
    "print(f\"{'Step':<25} {'Shape':<15} {'Features':<10} {'Técnicas':<10} {'Shifted':<10} {'Candlestick':<12}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for step in pipeline_steps:\n",
    "    step_name = step['step_name'][:23] + '...' if len(step['step_name']) > 25 else step['step_name']\n",
    "    shape_str = f\"{step['shape'][0]}x{step['shape'][1]}\"\n",
    "    print(f\"{step_name:<25} {shape_str:<15} {step['shape'][1]:<10} {step['technical_count']:<10} {step['shifted_count']:<10} {step['candlestick_count']:<12}\")\n",
    "\n",
    "# Análise de evolução\n",
    "print(f\"\\n📈 EVOLUÇÃO DAS FEATURES:\")\n",
    "for i in range(1, len(pipeline_steps)):\n",
    "    prev_step = pipeline_steps[i-1]\n",
    "    curr_step = pipeline_steps[i]\n",
    "    \n",
    "    change = curr_step['shape'][1] - prev_step['shape'][1]\n",
    "    change_str = f\"{change:+d}\" if change != 0 else \"0\"\n",
    "    \n",
    "    print(f\"   {prev_step['step_name']} → {curr_step['step_name']}: {change_str} features\")\n",
    "\n",
    "# Identificar penhascos\n",
    "print(f\"\\n🚨 IDENTIFICAÇÃO DE PENHASCOS:\")\n",
    "penhascos_encontrados = []\n",
    "\n",
    "for i in range(1, len(pipeline_steps)):\n",
    "    prev_step = pipeline_steps[i-1]\n",
    "    curr_step = pipeline_steps[i]\n",
    "    \n",
    "    loss = prev_step['shape'][1] - curr_step['shape'][1]\n",
    "    if loss > 5:  # Perda significativa\n",
    "        penhascos_encontrados.append({\n",
    "            'from': prev_step['step_name'],\n",
    "            'to': curr_step['step_name'],\n",
    "            'loss': loss\n",
    "        })\n",
    "\n",
    "if penhascos_encontrados:\n",
    "    print(f\"   ❌ {len(penhascos_encontrados)} penhasco(s) identificado(s):\")\n",
    "    for penhasco in penhascos_encontrados:\n",
    "        print(f\"      {penhasco['from']} → {penhasco['to']}: -{penhasco['loss']} features\")\n",
    "else:\n",
    "    print(f\"   ✅ Nenhum penhasco significativo identificado\")\n",
    "\n",
    "# Verificação final\n",
    "print(f\"\\n✅ VERIFICAÇÃO FINAL:\")\n",
    "print(f\"   Dataset inicial: {pipeline_steps[0]['shape']}\")\n",
    "print(f\"   Dataset final: {pipeline_steps[-1]['shape']}\")\n",
    "print(f\"   Features criadas: {pipeline_steps[-1]['shape'][1] - pipeline_steps[0]['shape'][1]:+d}\")\n",
    "print(f\"   Linhas preservadas: {pipeline_steps[-1]['shape'][0] / pipeline_steps[0]['shape'][0]:.1%}\")\n",
    "\n",
    "# Status do pipeline\n",
    "final_features = pipeline_steps[-1]['shape'][1]\n",
    "if final_features >= 50:\n",
    "    status = \"✅ EXCELENTE\"\n",
    "elif final_features >= 30:\n",
    "    status = \"✅ BOM\"\n",
    "elif final_features >= 20:\n",
    "    status = \"⚠️ ACEITÁVEL\"\n",
    "else:\n",
    "    status = \"❌ INSUFICIENTE\"\n",
    "\n",
    "print(f\"\\n🎯 STATUS DO PIPELINE: {status}\")\n",
    "print(f\"   Features finais: {final_features}\")\n",
    "print(f\"   Pronto para modelagem: {'SIM' if final_features >= 20 else 'NÃO'}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"🚀 PIPELINE EXPLÍCITO CONCLUÍDO COM SUCESSO!\")\n",
    "print(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORTAR DATASET FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 EXPORTANDO DATASET FINAL...\n",
      "✅ Dataset exportado: dataset_final_pipeline_explicito.csv\n",
      "📊 Shape: (3586, 49)\n",
      "📋 Features: ['Data', 'day_of_week', 'month', 'quarter', 'year', 'is_month_start', 'is_month_end', 'is_quarter_end', 'Target', 'Abertura_shifted', 'Máxima_shifted', 'Mínima_shifted', 'Último_shifted', 'Volume_shifted', 'MA_5_shifted', 'MA_10_shifted', 'MA_20_shifted', 'MA_50_shifted', 'BB_Upper_shifted', 'BB_Lower_shifted', 'BB_Width_shifted', 'BB_Position_shifted', 'RSI_shifted', 'MACD_shifted', 'Signal_Line_shifted', 'atr_5_shifted', 'atr_10_shifted', 'atr_20_shifted', 'volatility_5_shifted', 'volatility_10_shifted', 'volatility_20_shifted', 'Price_Range_shifted', 'Price_Position_shifted', 'Gap_shifted', 'hl_close_ratio_shifted', 'true_range_shifted', 'high_close_prev_shifted', 'low_close_prev_shifted', 'returns_shifted', 'Variacao_shifted', 'doji_prev', 'hammer_prev', 'shooting_star_prev', 'engulfing_bullish_prev', 'bullish_candle_prev', 'bearish_candle_prev', 'body_size_prev', 'upper_shadow_prev', 'lower_shadow_prev']\n",
      "✅ Relatório criado: relatorio_features_pipeline.txt\n",
      "\n",
      "🎉 PIPELINE EXPLÍCITO CONCLUÍDO COM SUCESSO!\n",
      "📊 49 features preservadas e prontas para modelagem\n"
     ]
    }
   ],
   "source": [
    "# EXPORTAR DATASET FINAL\n",
    "print(\"\\n💾 EXPORTANDO DATASET FINAL...\")\n",
    "\n",
    "# Salvar dataset final\n",
    "output_filename = 'dataset_final_pipeline_explicito.csv'\n",
    "df_step07_final.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"✅ Dataset exportado: {output_filename}\")\n",
    "print(f\"📊 Shape: {df_step07_final.shape}\")\n",
    "print(f\"📋 Features: {list(df_step07_final.columns)}\")\n",
    "\n",
    "# Criar relatório de features\n",
    "report_filename = 'relatorio_features_pipeline.txt'\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"RELATÓRIO DE FEATURES - PIPELINE EXPLÍCITO\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Dataset Final: {df_step07_final.shape}\\n\\n\")\n",
    "    \n",
    "    f.write(\"FEATURES POR CATEGORIA:\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    \n",
    "    # Categorizar features finais\n",
    "    final_cols = df_step07_final.columns\n",
    "    \n",
    "    ohlc_final = [col for col in final_cols if any(x in col for x in ['Abertura', 'Máxima', 'Mínima', 'Último', 'Volume'])]\n",
    "    temporal_final = [col for col in final_cols if any(x in col.lower() for x in ['data', 'day', 'month', 'quarter', 'year'])]\n",
    "    technical_final = [col for col in final_cols if any(x in col.lower() for x in ['ma_', 'bb_', 'rsi', 'atr', 'volatility', 'macd'])]\n",
    "    shifted_final = [col for col in final_cols if col.endswith('_shifted')]\n",
    "    candlestick_final = [col for col in final_cols if any(x in col.lower() for x in ['doji', 'hammer', 'engulf', 'prev'])]\n",
    "    target_final = [col for col in final_cols if 'target' in col.lower()]\n",
    "    \n",
    "    f.write(f\"OHLC/Volume ({len(ohlc_final)}): {ohlc_final}\\n\\n\")\n",
    "    f.write(f\"Temporais ({len(temporal_final)}): {temporal_final}\\n\\n\")\n",
    "    f.write(f\"Técnicas ({len(technical_final)}): {technical_final}\\n\\n\")\n",
    "    f.write(f\"Shifted ({len(shifted_final)}): {shifted_final}\\n\\n\")\n",
    "    f.write(f\"Candlestick ({len(candlestick_final)}): {candlestick_final}\\n\\n\")\n",
    "    f.write(f\"Target ({len(target_final)}): {target_final}\\n\\n\")\n",
    "\n",
    "print(f\"✅ Relatório criado: {report_filename}\")\n",
    "print(f\"\\n🎉 PIPELINE EXPLÍCITO CONCLUÍDO COM SUCESSO!\")\n",
    "print(f\"📊 {df_step07_final.shape[1]} features preservadas e prontas para modelagem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏆 OLIMPÍADA DE MODELOS: TESTE COMPLETO DE ALGORITMOS\n",
    "\n",
    "## Objetivo Duplo:\n",
    "1. **Encontrar o algoritmo de melhor performance**\n",
    "2. **Verificar se features de candlestick agregam valor preditivo**\n",
    "\n",
    "## Estratégia de Teste:\n",
    "- **Dataset A (Baseline)**: ~40 features (STEP 05 - sem candlestick)\n",
    "- **Dataset B (Enriquecido)**: ~49 features (STEP 07 - com candlestick)\n",
    "\n",
    "## Categorias de Modelos:\n",
    "1. **Baseline Linear**: Regressão Logística\n",
    "2. **Ensembles de Árvores**: Random Forest, XGBoost, LightGBM\n",
    "3. **Não-Lineares**: SVM (RBF)\n",
    "4. **Ensemble Híbrido**: Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 === OLIMPÍADA DE MODELOS === 🏆\n",
      "\n",
      "Preparando framework de testes completo...\n",
      "✅ Framework de avaliação preparado\n",
      "🏁 Pronto para iniciar a competição!\n"
     ]
    }
   ],
   "source": [
    "# PREPARAÇÃO PARA OLIMPÍADA DE MODELOS\n",
    "print(\"🏆 === OLIMPÍADA DE MODELOS === 🏆\")\n",
    "print()\n",
    "print(\"Preparando framework de testes completo...\")\n",
    "\n",
    "# Imports necessários\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Função para avaliação completa de modelos\n",
    "def evaluate_model_complete(model, X_train, X_test, y_train, y_test, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Avaliação completa de um modelo com múltiplas métricas\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predições\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "    \n",
    "    # Tempo de treinamento\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Cross-validation com TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='accuracy')\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'dataset': dataset_name,\n",
    "        'accuracy': accuracy,\n",
    "        'auc_score': auc_score,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'training_time': training_time,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "# Função para preparar datasets\n",
    "def prepare_datasets():\n",
    "    \"\"\"\n",
    "    Prepara Dataset A (baseline) e Dataset B (enriquecido)\n",
    "    \"\"\"\n",
    "    print(\"📊 Preparando datasets para competição...\")\n",
    "    \n",
    "    # Dataset A (Baseline) - STEP 05 (sem candlestick)\n",
    "    dataset_a = df_step05_temporal.copy()\n",
    "    \n",
    "    # Dataset B (Enriquecido) - STEP 07 (com candlestick)\n",
    "    dataset_b = df_step07_final.copy()\n",
    "    \n",
    "    print(f\"   Dataset A (Baseline): {dataset_a.shape}\")\n",
    "    print(f\"   Dataset B (Enriquecido): {dataset_b.shape}\")\n",
    "    print(f\"   Diferença: +{dataset_b.shape[1] - dataset_a.shape[1]} features no Dataset B\")\n",
    "    \n",
    "    return dataset_a, dataset_b\n",
    "\n",
    "# Função para split temporal\n",
    "def create_temporal_split(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Cria split temporal respeitando ordem cronológica\n",
    "    \"\"\"\n",
    "    # Ordenar por data se disponível\n",
    "    if 'Data' in df.columns:\n",
    "        df_sorted = df.sort_values('Data').reset_index(drop=True)\n",
    "    else:\n",
    "        df_sorted = df.copy()\n",
    "    \n",
    "    # Split temporal\n",
    "    split_idx = int(len(df_sorted) * (1 - test_size))\n",
    "    \n",
    "    train_df = df_sorted.iloc[:split_idx]\n",
    "    test_df = df_sorted.iloc[split_idx:]\n",
    "    \n",
    "    # Separar features e target\n",
    "    feature_cols = [col for col in df_sorted.columns if col not in ['Data', 'Target']]\n",
    "    \n",
    "    X_train = train_df[feature_cols]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_train = train_df['Target']\n",
    "    y_test = test_df['Target']\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"✅ Framework de avaliação preparado\")\n",
    "print(\"🏁 Pronto para iniciar a competição!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARAÇÃO DOS DATASETS E COMPETIDORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 === PREPARAÇÃO DOS DATASETS === 📊\n",
      "📊 Preparando datasets para competição...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_step05_temporal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📊 === PREPARAÇÃO DOS DATASETS === 📊\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Preparar datasets A e B\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m dataset_a, dataset_b = \u001b[43mprepare_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Criar splits temporais para ambos datasets\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔄 Criando splits temporais...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mprepare_datasets\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m📊 Preparando datasets para competição...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Dataset A (Baseline) - STEP 05 (sem candlestick)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m dataset_a = \u001b[43mdf_step05_temporal\u001b[49m.copy()\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Dataset B (Enriquecido) - STEP 07 (com candlestick)\u001b[39;00m\n\u001b[32m     70\u001b[39m dataset_b = df_step07_final.copy()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_step05_temporal' is not defined"
     ]
    }
   ],
   "source": [
    "# PREPARAÇÃO DOS DATASETS\n",
    "print(\"📊 === PREPARAÇÃO DOS DATASETS === 📊\")\n",
    "\n",
    "# Preparar datasets A e B\n",
    "dataset_a, dataset_b = prepare_datasets()\n",
    "\n",
    "# Criar splits temporais para ambos datasets\n",
    "print(f\"\\n🔄 Criando splits temporais...\")\n",
    "\n",
    "X_train_a, X_test_a, y_train_a, y_test_a = create_temporal_split(dataset_a)\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = create_temporal_split(dataset_b)\n",
    "\n",
    "print(f\"\\n📋 RESUMO DOS SPLITS:\")\n",
    "print(f\"Dataset A - Train: {X_train_a.shape}, Test: {X_test_a.shape}\")\n",
    "print(f\"Dataset B - Train: {X_train_b.shape}, Test: {X_test_b.shape}\")\n",
    "\n",
    "# Verificar distribuição do target\n",
    "print(f\"\\n🎯 DISTRIBUIÇÃO DO TARGET:\")\n",
    "print(f\"Dataset A - Train: {y_train_a.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"Dataset A - Test: {y_test_a.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"Dataset B - Train: {y_train_b.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"Dataset B - Test: {y_test_b.value_counts(normalize=True).to_dict()}\")\n",
    "\n",
    "# Preparar competidores\n",
    "print(f\"\\n🏆 === PREPARAÇÃO DOS COMPETIDORES === 🏆\")\n",
    "\n",
    "# Categoria 1: Baseline Linear\n",
    "competitors = {\n",
    "    'Regressão Logística': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(C=0.1, solver='liblinear', random_state=42))\n",
    "    ]),\n",
    "    \n",
    "    # Categoria 2: Ensembles de Árvores\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    \n",
    "    'LightGBM': lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    \n",
    "    # Categoria 3: Não-Lineares\n",
    "    'SVM (RBF)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', SVC(kernel='rbf', probability=True, random_state=42, C=1.0))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(f\"✅ {len(competitors)} competidores preparados:\")\n",
    "for name in competitors.keys():\n",
    "    print(f\"   🤖 {name}\")\n",
    "\n",
    "print(f\"\\n🚀 Tudo pronto para a Olimpíada de Modelos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXECUÇÃO DA OLIMPÍADA DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUÇÃO DA OLIMPÍADA DE MODELOS\n",
    "print(\"🏆 === INICIANDO OLIMPÍADA DE MODELOS === 🏆\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "# Armazenar todos os resultados\n",
    "all_results = []\n",
    "\n",
    "# Executar cada modelo em ambos datasets\n",
    "for model_name, model in competitors.items():\n",
    "    print(f\"\\n🤖 === TESTANDO: {model_name.upper()} === 🤖\")\n",
    "    \n",
    "    # Teste no Dataset A (Baseline)\n",
    "    print(f\"\\n📊 Dataset A (Baseline - {X_train_a.shape[1]} features):\")\n",
    "    try:\n",
    "        result_a = evaluate_model_complete(\n",
    "            model, X_train_a, X_test_a, y_train_a, y_test_a, \n",
    "            model_name, \"Dataset A (Baseline)\"\n",
    "        )\n",
    "        all_results.append(result_a)\n",
    "        \n",
    "        print(f\"   ✅ Accuracy: {result_a['accuracy']:.4f}\")\n",
    "        print(f\"   📈 AUC: {result_a['auc_score']:.4f}\" if result_a['auc_score'] else \"   📈 AUC: N/A\")\n",
    "        print(f\"   🔄 CV Mean: {result_a['cv_mean']:.4f} (±{result_a['cv_std']:.4f})\")\n",
    "        print(f\"   ⏱️ Tempo: {result_a['training_time']:.2f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Erro no Dataset A: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Teste no Dataset B (Enriquecido)\n",
    "    print(f\"\\n📊 Dataset B (Enriquecido - {X_train_b.shape[1]} features):\")\n",
    "    try:\n",
    "        result_b = evaluate_model_complete(\n",
    "            model, X_train_b, X_test_b, y_train_b, y_test_b, \n",
    "            model_name, \"Dataset B (Enriquecido)\"\n",
    "        )\n",
    "        all_results.append(result_b)\n",
    "        \n",
    "        print(f\"   ✅ Accuracy: {result_b['accuracy']:.4f}\")\n",
    "        print(f\"   📈 AUC: {result_b['auc_score']:.4f}\" if result_b['auc_score'] else \"   📈 AUC: N/A\")\n",
    "        print(f\"   🔄 CV Mean: {result_b['cv_mean']:.4f} (±{result_b['cv_std']:.4f})\")\n",
    "        print(f\"   ⏱️ Tempo: {result_b['training_time']:.2f}s\")\n",
    "        \n",
    "        # Comparar com Dataset A\n",
    "        accuracy_diff = result_b['accuracy'] - result_a['accuracy']\n",
    "        auc_diff = (result_b['auc_score'] - result_a['auc_score']) if (result_b['auc_score'] and result_a['auc_score']) else None\n",
    "        \n",
    "        print(f\"\\n📊 COMPARAÇÃO (B vs A):\")\n",
    "        print(f\"   📈 Accuracy: {accuracy_diff:+.4f} ({'✅ Melhora' if accuracy_diff > 0 else '❌ Piora' if accuracy_diff < 0 else '➖ Igual'})\")\n",
    "        if auc_diff is not None:\n",
    "            print(f\"   📈 AUC: {auc_diff:+.4f} ({'✅ Melhora' if auc_diff > 0 else '❌ Piora' if auc_diff < 0 else '➖ Igual'})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Erro no Dataset B: {e}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "\n",
    "print(f\"\\n🏁 OLIMPÍADA CONCLUÍDA!\")\n",
    "print(f\"📊 Total de resultados coletados: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANÁLISE COMPLETA DOS RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANÁLISE COMPLETA DOS RESULTADOS\n",
    "print(\"📊 === ANÁLISE COMPLETA DOS RESULTADOS === 📊\")\n",
    "print()\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    # Criar DataFrame com resultados\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Tabela de resultados\n",
    "    print(\"🏆 TABELA DE RESULTADOS COMPLETA:\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Modelo':<20} {'Dataset':<25} {'Accuracy':<10} {'AUC':<8} {'CV Mean':<10} {'CV Std':<8} {'Tempo(s)':<8}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        auc_str = f\"{row['auc_score']:.4f}\" if row['auc_score'] is not None else \"N/A\"\n",
    "        print(f\"{row['model_name']:<20} {row['dataset']:<25} {row['accuracy']:<10.4f} {auc_str:<8} {row['cv_mean']:<10.4f} {row['cv_std']:<8.4f} {row['training_time']:<8.2f}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Análise por dataset\n",
    "    print(\"📊 RANKING POR DATASET:\")\n",
    "    print()\n",
    "    \n",
    "    for dataset in ['Dataset A (Baseline)', 'Dataset B (Enriquecido)']:\n",
    "        dataset_results = results_df[results_df['dataset'] == dataset].copy()\n",
    "        if len(dataset_results) > 0:\n",
    "            dataset_results = dataset_results.sort_values('accuracy', ascending=False)\n",
    "            \n",
    "            print(f\"🏅 {dataset}:\")\n",
    "            for i, (_, row) in enumerate(dataset_results.iterrows(), 1):\n",
    "                medal = \"🥇\" if i == 1 else \"🥈\" if i == 2 else \"🥉\" if i == 3 else f\"{i}º\"\n",
    "                auc_str = f\"AUC: {row['auc_score']:.4f}\" if row['auc_score'] is not None else \"AUC: N/A\"\n",
    "                print(f\"   {medal} {row['model_name']}: Acc: {row['accuracy']:.4f}, {auc_str}\")\n",
    "            print()\n",
    "    \n",
    "    # Análise do impacto das features de candlestick\n",
    "    print(\"🕯️ ANÁLISE DO IMPACTO DAS FEATURES DE CANDLESTICK:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    candlestick_impact = []\n",
    "    \n",
    "    for model_name in results_df['model_name'].unique():\n",
    "        model_results = results_df[results_df['model_name'] == model_name]\n",
    "        \n",
    "        if len(model_results) == 2:  # Tem resultado para ambos datasets\n",
    "            baseline = model_results[model_results['dataset'] == 'Dataset A (Baseline)'].iloc[0]\n",
    "            enriched = model_results[model_results['dataset'] == 'Dataset B (Enriquecido)'].iloc[0]\n",
    "            \n",
    "            accuracy_improvement = enriched['accuracy'] - baseline['accuracy']\n",
    "            auc_improvement = (enriched['auc_score'] - baseline['auc_score']) if (enriched['auc_score'] and baseline['auc_score']) else None\n",
    "            \n",
    "            candlestick_impact.append({\n",
    "                'model': model_name,\n",
    "                'accuracy_improvement': accuracy_improvement,\n",
    "                'auc_improvement': auc_improvement,\n",
    "                'baseline_accuracy': baseline['accuracy'],\n",
    "                'enriched_accuracy': enriched['accuracy']\n",
    "            })\n",
    "            \n",
    "            impact_symbol = \"✅\" if accuracy_improvement > 0.01 else \"⚠️\" if accuracy_improvement > 0 else \"❌\"\n",
    "            print(f\"{impact_symbol} {model_name}:\")\n",
    "            print(f\"   Accuracy: {baseline['accuracy']:.4f} → {enriched['accuracy']:.4f} ({accuracy_improvement:+.4f})\")\n",
    "            if auc_improvement is not None:\n",
    "                print(f\"   AUC: {baseline['auc_score']:.4f} → {enriched['auc_score']:.4f} ({auc_improvement:+.4f})\")\n",
    "            print()\n",
    "    \n",
    "    # Resumo do impacto das features de candlestick\n",
    "    if candlestick_impact:\n",
    "        avg_accuracy_improvement = np.mean([x['accuracy_improvement'] for x in candlestick_impact])\n",
    "        positive_improvements = sum(1 for x in candlestick_impact if x['accuracy_improvement'] > 0)\n",
    "        \n",
    "        print(f\"📈 RESUMO DO IMPACTO DAS FEATURES DE CANDLESTICK:\")\n",
    "        print(f\"   Melhoria média de accuracy: {avg_accuracy_improvement:+.4f}\")\n",
    "        print(f\"   Modelos que melhoraram: {positive_improvements}/{len(candlestick_impact)}\")\n",
    "        \n",
    "        if avg_accuracy_improvement > 0.01:\n",
    "            conclusion = \"✅ FEATURES DE CANDLESTICK AGREGAM VALOR SIGNIFICATIVO\"\n",
    "        elif avg_accuracy_improvement > 0:\n",
    "            conclusion = \"⚠️ FEATURES DE CANDLESTICK AGREGAM VALOR MARGINAL\"\n",
    "        else:\n",
    "            conclusion = \"❌ FEATURES DE CANDLESTICK NÃO AGREGAM VALOR\"\n",
    "        \n",
    "        print(f\"\\n🎯 CONCLUSÃO: {conclusion}\")\n",
    "    \n",
    "    # Identificar o campeão geral\n",
    "    print(f\"\\n🏆 CAMPEÃO GERAL:\")\n",
    "    best_result = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "    print(f\"   🥇 {best_result['model_name']} no {best_result['dataset']}\")\n",
    "    print(f\"   📊 Accuracy: {best_result['accuracy']:.4f}\")\n",
    "    print(f\"   📈 AUC: {best_result['auc_score']:.4f}\" if best_result['auc_score'] else \"   📈 AUC: N/A\")\n",
    "    print(f\"   🔄 CV: {best_result['cv_mean']:.4f} (±{best_result['cv_std']:.4f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Nenhum resultado coletado. Verifique a execução dos modelos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLE HÍBRIDO: COMBINANDO OS MELHORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE HÍBRIDO: COMBINANDO OS MELHORES\n",
    "print(\"🤝 === ENSEMBLE HÍBRIDO: COMBINANDO OS MELHORES === 🤝\")\n",
    "print()\n",
    "\n",
    "if len(all_results) >= 3:\n",
    "    # Identificar os 3 melhores modelos no Dataset B\n",
    "    dataset_b_results = [r for r in all_results if r['dataset'] == 'Dataset B (Enriquecido)']\n",
    "    \n",
    "    if len(dataset_b_results) >= 3:\n",
    "        # Ordenar por accuracy\n",
    "        dataset_b_results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "        top_3 = dataset_b_results[:3]\n",
    "        \n",
    "        print(f\"🏅 TOP 3 MODELOS SELECIONADOS PARA ENSEMBLE:\")\n",
    "        for i, result in enumerate(top_3, 1):\n",
    "            print(f\"   {i}º {result['model_name']}: {result['accuracy']:.4f}\")\n",
    "        \n",
    "        # Criar Voting Classifier com os top 3\n",
    "        ensemble_estimators = []\n",
    "        \n",
    "        for result in top_3:\n",
    "            model_name = result['model_name']\n",
    "            model = result['model']\n",
    "            \n",
    "            # Criar nome único para o ensemble\n",
    "            ensemble_name = model_name.lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "            ensemble_estimators.append((ensemble_name, model))\n",
    "        \n",
    "        # Criar Voting Classifier\n",
    "        voting_classifier = VotingClassifier(\n",
    "            estimators=ensemble_estimators,\n",
    "            voting='soft'  # Usa probabilidades\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🤖 Testando Voting Classifier (Soft Voting)...\")\n",
    "        \n",
    "        # Testar ensemble no Dataset B\n",
    "        try:\n",
    "            ensemble_result = evaluate_model_complete(\n",
    "                voting_classifier, X_train_b, X_test_b, y_train_b, y_test_b,\n",
    "                \"Voting Classifier\", \"Dataset B (Enriquecido)\"\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n📊 RESULTADO DO ENSEMBLE:\")\n",
    "            print(f\"   ✅ Accuracy: {ensemble_result['accuracy']:.4f}\")\n",
    "            print(f\"   📈 AUC: {ensemble_result['auc_score']:.4f}\")\n",
    "            print(f\"   🔄 CV Mean: {ensemble_result['cv_mean']:.4f} (±{ensemble_result['cv_std']:.4f})\")\n",
    "            print(f\"   ⏱️ Tempo: {ensemble_result['training_time']:.2f}s\")\n",
    "            \n",
    "            # Comparar com o melhor individual\n",
    "            best_individual = top_3[0]\n",
    "            improvement = ensemble_result['accuracy'] - best_individual['accuracy']\n",
    "            \n",
    "            print(f\"\\n🆚 ENSEMBLE vs MELHOR INDIVIDUAL:\")\n",
    "            print(f\"   Melhor individual: {best_individual['model_name']} ({best_individual['accuracy']:.4f})\")\n",
    "            print(f\"   Ensemble: {ensemble_result['accuracy']:.4f}\")\n",
    "            print(f\"   Diferença: {improvement:+.4f} ({'✅ Melhora' if improvement > 0 else '❌ Piora' if improvement < 0 else '➖ Igual'})\")\n",
    "            \n",
    "            # Adicionar resultado do ensemble à lista\n",
    "            all_results.append(ensemble_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao criar ensemble: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"⚠️ Poucos resultados no Dataset B para criar ensemble\")\n",
    "else:\n",
    "    print(f\"⚠️ Poucos resultados coletados para criar ensemble\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZAÇÕES E RELATÓRIO FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZAÇÕES E RELATÓRIO FINAL\n",
    "print(\"📊 === VISUALIZAÇÕES E RELATÓRIO FINAL === 📊\")\n",
    "print()\n",
    "\n",
    "if len(all_results) > 0:\n",
    "    # Criar DataFrame atualizado com ensemble\n",
    "    final_results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Gráfico de comparação de accuracy\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Subplot 1: Comparação de Accuracy por Modelo e Dataset\n",
    "    plt.subplot(2, 2, 1)\n",
    "    \n",
    "    # Preparar dados para o gráfico\n",
    "    models = final_results_df['model_name'].unique()\n",
    "    datasets = final_results_df['dataset'].unique()\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        dataset_data = final_results_df[final_results_df['dataset'] == dataset]\n",
    "        accuracies = []\n",
    "        \n",
    "        for model in models:\n",
    "            model_data = dataset_data[dataset_data['model_name'] == model]\n",
    "            if len(model_data) > 0:\n",
    "                accuracies.append(model_data.iloc[0]['accuracy'])\n",
    "            else:\n",
    "                accuracies.append(0)\n",
    "        \n",
    "        plt.bar(x + i*width, accuracies, width, label=dataset, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Modelos')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Comparação de Accuracy por Modelo e Dataset')\n",
    "    plt.xticks(x + width/2, models, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Impacto das Features de Candlestick\n",
    "    plt.subplot(2, 2, 2)\n",
    "    \n",
    "    if len(candlestick_impact) > 0:\n",
    "        models_impact = [x['model'] for x in candlestick_impact]\n",
    "        improvements = [x['accuracy_improvement'] for x in candlestick_impact]\n",
    "        \n",
    "        colors = ['green' if x > 0 else 'red' for x in improvements]\n",
    "        plt.bar(models_impact, improvements, color=colors, alpha=0.7)\n",
    "        plt.xlabel('Modelos')\n",
    "        plt.ylabel('Melhoria de Accuracy')\n",
    "        plt.title('Impacto das Features de Candlestick')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Distribuição de AUC\n",
    "    plt.subplot(2, 2, 3)\n",
    "    \n",
    "    auc_scores = [r['auc_score'] for r in all_results if r['auc_score'] is not None]\n",
    "    if auc_scores:\n",
    "        plt.hist(auc_scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.xlabel('AUC Score')\n",
    "        plt.ylabel('Frequência')\n",
    "        plt.title('Distribuição de AUC Scores')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 4: Tempo de Treinamento\n",
    "    plt.subplot(2, 2, 4)\n",
    "    \n",
    "    model_names = [r['model_name'] for r in all_results]\n",
    "    training_times = [r['training_time'] for r in all_results]\n",
    "    \n",
    "    plt.scatter(training_times, [r['accuracy'] for r in all_results], alpha=0.7, s=100)\n",
    "    \n",
    "    for i, model in enumerate(model_names):\n",
    "        plt.annotate(model[:10], (training_times[i], all_results[i]['accuracy']), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    plt.xlabel('Tempo de Treinamento (s)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy vs Tempo de Treinamento')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Relatório final em texto\n",
    "    print(f\"\\n📋 === RELATÓRIO FINAL DA OLIMPÍADA === 📋\")\n",
    "    print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    # Campeão absoluto\n",
    "    best_overall = final_results_df.loc[final_results_df['accuracy'].idxmax()]\n",
    "    print(f\"\\n🏆 CAMPEÃO ABSOLUTO:\")\n",
    "    print(f\"   Modelo: {best_overall['model_name']}\")\n",
    "    print(f\"   Dataset: {best_overall['dataset']}\")\n",
    "    print(f\"   Accuracy: {best_overall['accuracy']:.4f}\")\n",
    "    print(f\"   AUC: {best_overall['auc_score']:.4f}\" if best_overall['auc_score'] else \"   AUC: N/A\")\n",
    "    \n",
    "    # Melhor por categoria\n",
    "    print(f\"\\n🏅 MELHORES POR CATEGORIA:\")\n",
    "    \n",
    "    categories = {\n",
    "        'Linear': ['Regressão Logística'],\n",
    "        'Árvores': ['Random Forest', 'XGBoost', 'LightGBM'],\n",
    "        'Não-Linear': ['SVM (RBF)'],\n",
    "        'Ensemble': ['Voting Classifier']\n",
    "    }\n",
    "    \n",
    "    for category, models in categories.items():\n",
    "        category_results = final_results_df[final_results_df['model_name'].isin(models)]\n",
    "        if len(category_results) > 0:\n",
    "            best_in_category = category_results.loc[category_results['accuracy'].idxmax()]\n",
    "            print(f\"   {category}: {best_in_category['model_name']} ({best_in_category['accuracy']:.4f})\")\n",
    "    \n",
    "    # Conclusão sobre features de candlestick\n",
    "    if candlestick_impact:\n",
    "        avg_improvement = np.mean([x['accuracy_improvement'] for x in candlestick_impact])\n",
    "        print(f\"\\n🕯️ CONCLUSÃO SOBRE FEATURES DE CANDLESTICK:\")\n",
    "        print(f\"   Melhoria média: {avg_improvement:+.4f}\")\n",
    "        \n",
    "        if avg_improvement > 0.01:\n",
    "            print(f\"   ✅ RECOMENDAÇÃO: Usar features de candlestick (melhoria significativa)\")\n",
    "        elif avg_improvement > 0:\n",
    "            print(f\"   ⚠️ RECOMENDAÇÃO: Considerar features de candlestick (melhoria marginal)\")\n",
    "        else:\n",
    "            print(f\"   ❌ RECOMENDAÇÃO: Não usar features de candlestick (sem benefício)\")\n",
    "    \n",
    "    # Recomendação final\n",
    "    print(f\"\\n🎯 RECOMENDAÇÃO FINAL PARA PRODUÇÃO:\")\n",
    "    print(f\"   Modelo: {best_overall['model_name']}\")\n",
    "    print(f\"   Dataset: {best_overall['dataset']}\")\n",
    "    print(f\"   Justificativa: Melhor accuracy ({best_overall['accuracy']:.4f}) na competição\")\n",
    "    \n",
    "    print(f\"\\n🎉 OLIMPÍADA DE MODELOS CONCLUÍDA COM SUCESSO!\")\n",
    "    print(f\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Nenhum resultado disponível para análise\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
