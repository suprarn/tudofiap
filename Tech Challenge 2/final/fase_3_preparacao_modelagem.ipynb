{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 3: Prepara√ß√£o da Base para Modelagem (EAP 3.0)\n",
    "\n",
    "Este notebook implementa a Fase 3 do projeto de previs√£o de tend√™ncia do IBOVESPA, estruturando os dados para que possam ser consumidos pelos algoritmos de machine learning, respeitando a ordem temporal.\n",
    "\n",
    "**Autor:** Projeto Tech Challenge 2  \n",
    "**Data:** 2025-01-24  \n",
    "**Refer√™ncia:** EAP.md e Steering.md\n",
    "\n",
    "## üéØ Objetivos da Fase 3:\n",
    "\n",
    "1. **Estrutura√ß√£o com Janela Deslizante**: Transformar s√©rie temporal em dataset tabular\n",
    "2. **Divis√£o Cronol√≥gica**: Separar dados respeitando ordem temporal\n",
    "3. **Escalonamento**: Normalizar features para melhor performance dos modelos\n",
    "\n",
    "## ‚ö†Ô∏è Abordagem Funcional\n",
    "\n",
    "**Mudan√ßa de arquitetura**: Convertemos a estrutura de classe para **abordagem funcional** mais adequada ao formato notebook, facilitando a compreens√£o e execu√ß√£o sequencial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√µes e Configura√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Verifica vers√µes\n",
    "print('=== VERIFICA√á√ÉO DE COMPATIBILIDADE ===')\n",
    "print(f'‚úì Pandas: {pd.__version__}')\n",
    "print(f'‚úì NumPy: {np.__version__}')\n",
    "print(f'‚úì Scikit-learn: Importado com sucesso')\n",
    "print('‚úì Bibliotecas importadas com sucesso!')\n",
    "print('‚úì Pronto para prepara√ß√£o dos dados para modelagem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento dos Dados da Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados processados da Fase 2\n",
    "try:\n",
    "    dados_com_features = pd.read_csv('dados_fase2_completos.csv', index_col=0, parse_dates=True)\n",
    "    print('‚úì Dados da Fase 2 carregados com sucesso!')\nexcept FileNotFoundError:\n",
    "    print('‚ö†Ô∏è Arquivo dados_fase2_completos.csv n√£o encontrado.')\n",
    "    print('‚ö†Ô∏è Executando a Fase 2 primeiro ou carregando dados alternativos...')\n",
    "    # Carrega dados b√°sicos como fallback\n",
    "    dados_com_features = pd.read_csv('dados_bovespa.csv', index_col=0, parse_dates=True)\n",
    "    # Cria target simples para demonstra√ß√£o\n",
    "    col_close = [col for col in dados_com_features.columns if 'close' in col.lower()][0]\n",
    "    dados_com_features['Target'] = (dados_com_features[col_close].shift(-1) > dados_com_features[col_close]).astype(int)\n",
    "    dados_com_features = dados_com_features.dropna()\n",
    "    print('‚úì Dados b√°sicos carregados e target criado')\n",
    "\n",
    "print(f'\\n=== INFORMA√á√ïES DOS DADOS ===')\n",
    "print(f'‚úì Registros: {len(dados_com_features)}')\n",
    "print(f'‚úì Per√≠odo: {dados_com_features.index.min()} a {dados_com_features.index.max()}')\n",
    "print(f'‚úì Colunas: {len(dados_com_features.columns)}')\n",
    "print(f'‚úì Colunas dispon√≠veis: {list(dados_com_features.columns)}')\n",
    "\n",
    "# Verifica se h√° coluna Target\n",
    "if 'Target' not in dados_com_features.columns:\n",
    "    raise ValueError('Coluna Target n√£o encontrada. Execute a Fase 2 primeiro.')\n",
    "\n",
    "# Visualiza as primeiras linhas\n",
    "print('\\n=== PRIMEIRAS LINHAS DOS DADOS ===')\n",
    "dados_com_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estrutura√ß√£o com Janela Deslizante\n",
    "\n",
    "### 3.1 Defini√ß√£o da Janela de Entrada (Lookback Window)\n",
    "\n",
    "Transformamos a s√©rie temporal em um dataset tabular onde cada linha cont√©m os atributos dos √∫ltimos **n=5 dias** e o alvo correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes da janela deslizante\n",
    "JANELA_TAMANHO = 5  # Lookback window de 5 dias\n",
    "\n",
    "print(f'=== ESTRUTURA√á√ÉO COM JANELA DESLIZANTE ===')\n",
    "print(f'‚úì Tamanho da janela: {JANELA_TAMANHO} dias')\n",
    "\n",
    "# Identifica features num√©ricas (exclui Target)\n",
    "colunas_excluir = ['Target']\n",
    "features_numericas = dados_com_features.select_dtypes(include=[np.number]).columns\n",
    "features_numericas = [col for col in features_numericas if col not in colunas_excluir]\n",
    "\n",
    "print(f'‚úì Features selecionadas: {len(features_numericas)}')\n",
    "print(f'‚úì Features: {features_numericas[:10]}...')  # Mostra apenas as primeiras 10\n",
    "\n",
    "# Remove linhas com NaN nas features\n",
    "dados_limpos = dados_com_features[features_numericas + ['Target']].dropna()\n",
    "print(f'‚úì Dados limpos: {len(dados_limpos)} registros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementa√ß√£o da L√≥gica de Janela Deslizante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria estrutura de janela deslizante\n",
    "print('=== CRIANDO ESTRUTURA DE JANELA DESLIZANTE ===')\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "indices_list = []\n",
    "\n",
    "# Itera pelos dados criando janelas deslizantes\n",
    "for i in range(JANELA_TAMANHO, len(dados_limpos)):\n",
    "    # Janela de features (√∫ltimos n dias)\n",
    "    janela_features = dados_limpos[features_numericas].iloc[i-JANELA_TAMANHO:i].values\n",
    "    \n",
    "    # Achata a janela (transforma matriz em vetor)\n",
    "    janela_achatada = janela_features.flatten()\n",
    "    \n",
    "    # Target correspondente\n",
    "    target = dados_limpos['Target'].iloc[i]\n",
    "    \n",
    "    X_list.append(janela_achatada)\n",
    "    y_list.append(target)\n",
    "    indices_list.append(dados_limpos.index[i])\n",
    "\n",
    "# Converte para arrays numpy\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)\n",
    "\n",
    "print(f'‚úì Janelas criadas: {len(X_list)}')\n",
    "print(f'‚úì Shape do X: {X.shape}')\n",
    "print(f'‚úì Shape do y: {y.shape}')\n",
    "\n",
    "# Cria nomes das colunas para o DataFrame final\n",
    "nomes_colunas = []\n",
    "for lag in range(JANELA_TAMANHO, 0, -1):\n",
    "    for feature in features_numericas:\n",
    "        nomes_colunas.append(f\"{feature}_lag_{lag}\")\n",
    "\n",
    "print(f'‚úì Colunas criadas: {len(nomes_colunas)}')\n",
    "print(f'‚úì Exemplo de colunas: {nomes_colunas[:5]}...')\n",
    "\n",
    "# Cria DataFrame estruturado\n",
    "dados_janela_deslizante = pd.DataFrame(\n",
    "    X, \n",
    "    columns=nomes_colunas,\n",
    "    index=indices_list\n",
    ")\n",
    "dados_janela_deslizante['Target'] = y\n",
    "\n",
    "print(f'\\n‚úì Dataset estruturado criado: {X.shape[0]} amostras, {X.shape[1]} features')\n",
    "print(f'‚úì Cada amostra cont√©m {JANELA_TAMANHO} dias de {len(features_numericas)} features')\n",
    "\n",
    "# Visualiza as primeiras linhas\n",
    "print('\\n=== PRIMEIRAS LINHAS DO DATASET ESTRUTURADO ===')\n",
    "dados_janela_deslizante.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Divis√£o Cronol√≥gica dos Dados\n",
    "\n",
    "### 4.1 Defini√ß√£o da Data de Corte\n",
    "\n",
    "Separamos os dados em treino (80%) e teste (20%) **sem usar amostragem aleat√≥ria**, respeitando a ordem cronol√≥gica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√µes da divis√£o\n",
    "PROPORCAO_TREINO = 0.8\n",
    "\n",
    "print(f'=== DIVIS√ÉO CRONOL√ìGICA DOS DADOS ===')\n",
    "print(f'‚úì Propor√ß√£o treino/teste: {PROPORCAO_TREINO:.0%}/{1-PROPORCAO_TREINO:.0%}')\n",
    "\n",
    "# Calcula ponto de corte cronol√≥gico\n",
    "total_amostras = len(dados_janela_deslizante)\n",
    "ponto_corte = int(total_amostras * PROPORCAO_TREINO)\n",
    "\n",
    "# Data de corte\n",
    "data_corte = dados_janela_deslizante.index[ponto_corte]\n",
    "print(f'‚úì Data de corte: {data_corte}')\n",
    "print(f'‚úì Ponto de corte: {ponto_corte} (de {total_amostras} amostras)')\n",
    "\n",
    "# Separa√ß√£o cronol√≥gica\n",
    "dados_treino = dados_janela_deslizante.iloc[:ponto_corte]\n",
    "dados_teste = dados_janela_deslizante.iloc[ponto_corte:]\n",
    "\n",
    "print(f'\\n‚úì Per√≠odo de treino: {dados_treino.index.min()} at√© {dados_treino.index.max()}')\n",
    "print(f'‚úì Per√≠odo de teste: {dados_teste.index.min()} at√© {dados_teste.index.max()}')\n",
    "print(f'‚úì Amostras de treino: {len(dados_treino)}')\n",
    "print(f'‚úì Amostras de teste: {len(dados_teste)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Separa√ß√£o de Features e Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa features e target\n",
    "colunas_features = [col for col in dados_janela_deslizante.columns if col != 'Target']\n",
    "\n",
    "X_train = dados_treino[colunas_features]\n",
    "y_train = dados_treino['Target']\n",
    "X_test = dados_teste[colunas_features]\n",
    "y_test = dados_teste['Target']\n",
    "\n",
    "print(f'=== SEPARA√á√ÉO DE FEATURES E TARGET ===')\n",
    "print(f'‚úì Features: {len(colunas_features)}')\n",
    "print(f'‚úì X_train shape: {X_train.shape}')\n",
    "print(f'‚úì y_train shape: {y_train.shape}')\n",
    "print(f'‚úì X_test shape: {X_test.shape}')\n",
    "print(f'‚úì y_test shape: {y_test.shape}')\n",
    "\n",
    "# Verifica distribui√ß√£o de classes\n",
    "dist_treino = y_train.value_counts(normalize=True)\n",
    "dist_teste = y_test.value_counts(normalize=True)\n",
    "\n",
    "print(f'\\n‚úì Distribui√ß√£o de classes no treino:')\n",
    "for classe, prop in dist_treino.items():\n",
    "    print(f'   - Classe {classe}: {prop:.2%}')\n",
    "\n",
    "print(f'‚úì Distribui√ß√£o de classes no teste:')\n",
    "for classe, prop in dist_teste.items():\n",
    "    print(f'   - Classe {classe}: {prop:.2%}')\n",
    "\n",
    "# Visualiza√ß√£o gr√°fica da distribui√ß√£o\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Distribui√ß√£o no treino\n",
    "dist_treino.plot(kind='bar', ax=ax1, color=['red', 'green'])\n",
    "ax1.set_title('Distribui√ß√£o de Classes - Treino')\n",
    "ax1.set_xlabel('Classe Target')\n",
    "ax1.set_ylabel('Propor√ß√£o')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Distribui√ß√£o no teste\n",
    "dist_teste.plot(kind='bar', ax=ax2, color=['red', 'green'])\n",
    "ax2.set_title('Distribui√ß√£o de Classes - Teste')\n",
    "ax2.set_xlabel('Classe Target')\n",
    "ax2.set_ylabel('Propor√ß√£o')\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Escalonamento de Atributos\n",
    "\n",
    "### 5.1 Instancia√ß√£o e Ajuste do StandardScaler\n",
    "\n",
    "**Importante**: O scaler √© ajustado **APENAS** no conjunto de treino para evitar data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=== ESCALONAMENTO DE ATRIBUTOS ===')\n",
    "\n",
    "# 5.1.1 - Instancia StandardScaler\n",
    "scaler = StandardScaler()\n",
    "print('‚úì StandardScaler instanciado')\n",
    "\n",
    "# 5.1.2 - Ajusta APENAS no conjunto de treino\n",
    "scaler.fit(X_train)\n",
    "print('‚úì Scaler ajustado APENAS nos dados de treino')\n",
    "print('‚úì Isso previne data leakage!')\n",
    "\n",
    "# Mostra estat√≠sticas do scaler\n",
    "print(f'\\n‚úì Estat√≠sticas do scaler (baseadas no treino):')\n",
    "print(f'   - N√∫mero de features: {len(scaler.mean_)}')\n",
    "print(f'   - M√©dia das primeiras 5 features: {scaler.mean_[:5]}')\n",
    "print(f'   - Desvio padr√£o das primeiras 5 features: {scaler.scale_[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Aplica√ß√£o da Transforma√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2.1 - Aplica transforma√ß√£o em ambos os conjuntos\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f'=== APLICA√á√ÉO DA TRANSFORMA√á√ÉO ===')\n",
    "print('‚úì Transforma√ß√£o aplicada em treino e teste')\n",
    "print('‚úì Usando os mesmos par√¢metros ajustados no treino')\n",
    "\n",
    "# Converte de volta para DataFrame mantendo √≠ndices e nomes das colunas\n",
    "X_train_scaled_df = pd.DataFrame(\n",
    "    X_train_scaled,\n",
    "    index=X_train.index,\n",
    "    columns=X_train.columns\n",
    ")\n",
    "\n",
    "X_test_scaled_df = pd.DataFrame(\n",
    "    X_test_scaled,\n",
    "    index=X_test.index,\n",
    "    columns=X_test.columns\n",
    ")\n",
    "\n",
    "print(f'\\n‚úì DataFrames escalonados criados:')\n",
    "print(f'   - X_train_scaled shape: {X_train_scaled_df.shape}')\n",
    "print(f'   - X_test_scaled shape: {X_test_scaled_df.shape}')\n",
    "\n",
    "# Verifica se a transforma√ß√£o foi aplicada corretamente\n",
    "media_treino = X_train_scaled_df.mean().mean()\n",
    "std_treino = X_train_scaled_df.std().mean()\n",
    "\n",
    "print(f'\\n‚úì Verifica√ß√£o da transforma√ß√£o:')\n",
    "print(f'   - M√©dia do treino escalonado: {media_treino:.6f} (deve ser ~0)')\n",
    "print(f'   - Desvio padr√£o do treino escalonado: {std_treino:.6f} (deve ser ~1)')\n",
    "\n",
    "if abs(media_treino) < 1e-10 and abs(std_treino - 1) < 0.1:\n",
    "    print('‚úÖ Escalonamento aplicado corretamente!')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Verificar escalonamento - valores fora do esperado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualiza√ß√£o dos Dados Preparados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=== VISUALIZA√á√ÉO DOS DADOS PREPARADOS ===')\n",
    "\n",
    "# Compara√ß√£o antes e depois do escalonamento\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Primeira feature - antes do escalonamento\n",
    "primeira_feature = X_train.columns[0]\n",
    "axes[0, 0].hist(X_train[primeira_feature], bins=30, alpha=0.7, color='blue')\n",
    "axes[0, 0].set_title(f'Antes do Escalonamento\\n{primeira_feature}')\n",
    "axes[0, 0].set_xlabel('Valor')\n",
    "axes[0, 0].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "# Primeira feature - depois do escalonamento\n",
    "axes[0, 1].hist(X_train_scaled_df[primeira_feature], bins=30, alpha=0.7, color='green')\n",
    "axes[0, 1].set_title(f'Depois do Escalonamento\\n{primeira_feature}')\n",
    "axes[0, 1].set_xlabel('Valor Escalonado')\n",
    "axes[0, 1].set_ylabel('Frequ√™ncia')\n",
    "\n",
    "# Distribui√ß√£o geral - antes\n",
    "sample_features = X_train.iloc[:, :5]  # Primeiras 5 features\n",
    "axes[1, 0].boxplot([sample_features[col].dropna() for col in sample_features.columns])\n",
    "axes[1, 0].set_title('Distribui√ß√£o Geral - Antes')\n",
    "axes[1, 0].set_xlabel('Features (primeiras 5)')\n",
    "axes[1, 0].set_ylabel('Valor')\n",
    "\n",
    "# Distribui√ß√£o geral - depois\n",
    "sample_features_scaled = X_train_scaled_df.iloc[:, :5]  # Primeiras 5 features\n",
    "axes[1, 1].boxplot([sample_features_scaled[col].dropna() for col in sample_features_scaled.columns])\n",
    "axes[1, 1].set_title('Distribui√ß√£o Geral - Depois')\n",
    "axes[1, 1].set_xlabel('Features (primeiras 5)')\n",
    "axes[1, 1].set_ylabel('Valor Escalonado')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostra estat√≠sticas descritivas\n",
    "print('\\n=== ESTAT√çSTICAS DESCRITIVAS ===')\n",
    "print('\\nüìä Dados ANTES do escalonamento (primeiras 5 features):')\n",
    "print(X_train.iloc[:, :5].describe())\n",
    "\n",
    "print('\\nüìä Dados DEPOIS do escalonamento (primeiras 5 features):')\n",
    "print(X_train_scaled_df.iloc[:, :5].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Salvamento dos Dados Preparados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=== SALVAMENTO DOS DADOS PREPARADOS ===')\n",
    "\n",
    "# Salva os dados preparados\n",
    "X_train.to_csv('X_train_fase3.csv')\n",
    "X_test.to_csv('X_test_fase3.csv')\n",
    "y_train.to_csv('y_train_fase3.csv')\n",
    "y_test.to_csv('y_test_fase3.csv')\n",
    "\n",
    "# Salva os dados escalonados\n",
    "X_train_scaled_df.to_csv('X_train_scaled_fase3.csv')\n",
    "X_test_scaled_df.to_csv('X_test_scaled_fase3.csv')\n",
    "\n",
    "# Salva informa√ß√µes da prepara√ß√£o\n",
    "info_preparacao = {\n",
    "    'janela_tamanho': JANELA_TAMANHO,\n",
    "    'proporcao_treino': PROPORCAO_TREINO,\n",
    "    'data_corte': str(data_corte),\n",
    "    'total_features': len(colunas_features),\n",
    "    'amostras_treino': len(X_train),\n",
    "    'amostras_teste': len(X_test),\n",
    "    'features_originais': len(features_numericas),\n",
    "    'periodo_treino': f'{X_train.index.min()} at√© {X_train.index.max()}',\n",
    "    'periodo_teste': f'{X_test.index.min()} at√© {X_test.index.max()}'\n",
    "}\n",
    "\n",
    "with open('info_preparacao_fase3.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(info_preparacao, f, indent=2, ensure_ascii=False, default=str)\n",
    "\n",
    "print('‚úì Arquivos salvos:')\n",
    "print('   - X_train_fase3.csv')\n",
    "print('   - X_test_fase3.csv')\n",
    "print('   - y_train_fase3.csv')\n",
    "print('   - y_test_fase3.csv')\n",
    "print('   - X_train_scaled_fase3.csv')\n",
    "print('   - X_test_scaled_fase3.csv')\n",
    "print('   - info_preparacao_fase3.json')\n",
    "\n",
    "print(f'\\n‚úÖ Dados preparados e salvos com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resumo da Fase 3\n",
    "\n",
    "### ‚úÖ Objetivos Alcan√ßados:\n",
    "\n",
    "1. **‚úÖ Estrutura√ß√£o com Janela Deslizante**: \n",
    "   - Janela de 5 dias implementada\n",
    "   - S√©rie temporal transformada em dataset tabular\n",
    "   - Cada amostra cont√©m hist√≥rico de 5 dias\n",
    "\n",
    "2. **‚úÖ Divis√£o Cronol√≥gica dos Dados**:\n",
    "   - Divis√£o 80/20 respeitando ordem temporal\n",
    "   - Sem amostragem aleat√≥ria\n",
    "   - Data de corte bem definida\n",
    "   - Distribui√ß√£o de classes preservada\n",
    "\n",
    "3. **‚úÖ Escalonamento de Atributos**:\n",
    "   - StandardScaler ajustado apenas no treino\n",
    "   - Preven√ß√£o de data leakage\n",
    "   - Transforma√ß√£o aplicada em treino e teste\n",
    "   - Verifica√ß√£o de qualidade realizada\n",
    "\n",
    "4. **‚úÖ Abordagem Funcional**:\n",
    "   - C√≥digo sem classes, mais direto\n",
    "   - Adequado ao formato notebook\n",
    "   - F√°cil compreens√£o e execu√ß√£o\n",
    "   - Compat√≠vel com NumPy 2.0+\n",
    "\n",
    "### üìä Dados Finais Preparados:\n",
    "\n",
    "- **Treino**: {len(X_train)} amostras com {len(colunas_features)} features\n",
    "- **Teste**: {len(X_test)} amostras com {len(colunas_features)} features\n",
    "- **Janela**: {JANELA_TAMANHO} dias de hist√≥rico por amostra\n",
    "- **Features**: Dados escalonados (m√©dia‚âà0, std‚âà1)\n",
    "- **Target**: Balanceamento preservado entre treino e teste\n",
    "\n",
    "### üéØ Pr√≥ximos Passos:\n",
    "\n",
    "1. **Fase 4**: Treinamento e valida√ß√£o de modelos de ML\n",
    "2. **Modelos a testar**: XGBoost, Random Forest, SVM, etc.\n",
    "3. **Valida√ß√£o**: Time Series Cross-Validation\n",
    "4. **M√©tricas**: Precis√£o, Recall, F1-Score, AUC-ROC\n",
    "\n",
    "### üí° Pontos Importantes:\n",
    "\n",
    "- **‚úÖ Data Leakage Prevenido**: Scaler ajustado apenas no treino\n",
    "- **‚úÖ Ordem Temporal Respeitada**: Divis√£o cronol√≥gica sem aleatoriedade\n",
    "- **‚úÖ Estrutura Adequada**: Janela deslizante para capturar padr√µes temporais\n",
    "- **‚úÖ Dados Prontos**: Formato ideal para algoritmos de ML\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Os dados est√£o prontos para a modelagem na Fase 4!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
