{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 4: Modelagem e Validação (EAP 4.0)\n",
    "\n",
    "Este notebook implementa a Fase 4 do projeto de previsão de tendência do IBOVESPA, focando no treinamento dos modelos, na sua avaliação rigorosa e na validação da robustez dos resultados.\n",
    "\n",
    "**Autor:** Projeto Tech Challenge 2  \n",
    "**Data:** 2025-01-24  \n",
    "**Referência:** EAP.md e Steering.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações e Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Bibliotecas importadas com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos Dados Preparados\n",
    "\n",
    "**Nota:** Substitua esta célula pelo carregamento dos seus dados preparados da Fase 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSTITUA ESTA CÉLULA PELO CARREGAMENTO DOS SEUS DADOS\n",
    "# Exemplo de como os dados devem estar estruturados:\n",
    "# dados_preparados = {\n",
    "#     'X_train_scaled': X_train_scaled,\n",
    "#     'y_train': y_train,\n",
    "#     'X_test_scaled': X_test_scaled,\n",
    "#     'y_test': y_test\n",
    "# }\n",
    "\n",
    "# Para demonstração, vamos criar dados fictícios\n",
    "# REMOVA ESTA SEÇÃO E USE SEUS DADOS REAIS\n",
    "np.random.seed(42)\n",
    "n_samples_train = 1000\n",
    "n_samples_test = 200\n",
    "n_features = 10\n",
    "\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    np.random.randn(n_samples_train, n_features),\n",
    "    columns=[f'feature_{i}' for i in range(n_features)]\n",
    ")\n",
    "y_train = pd.Series(np.random.choice([0, 1], n_samples_train, p=[0.6, 0.4]))\n",
    "\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    np.random.randn(n_samples_test, n_features),\n",
    "    columns=[f'feature_{i}' for i in range(n_features)]\n",
    ")\n",
    "y_test = pd.Series(np.random.choice([0, 1], n_samples_test, p=[0.6, 0.4]))\n",
    "\n",
    "dados_preparados = {\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'y_train': y_train,\n",
    "    'X_test_scaled': X_test_scaled,\n",
    "    'y_test': y_test\n",
    "}\n",
    "\n",
    "print(\"✓ Dados carregados\")\n",
    "print(f\"✓ Amostras de treino: {len(X_train_scaled)}\")\n",
    "print(f\"✓ Amostras de teste: {len(X_test_scaled)}\")\n",
    "print(f\"✓ Número de features: {X_train_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Treinamento do Modelo Baseline (Regressão Logística)\n",
    "\n",
    "### 4.1.1 - Instanciar e treinar um modelo de Regressão Logística\n",
    "### 4.1.2 - Realizar previsões no conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TREINAMENTO DO MODELO BASELINE (REGRESSÃO LOGÍSTICA) ===\")\n",
    "\n",
    "# Extrai dados\n",
    "X_train = dados_preparados['X_train_scaled']\n",
    "y_train = dados_preparados['y_train']\n",
    "X_test = dados_preparados['X_test_scaled']\n",
    "y_test = dados_preparados['y_test']\n",
    "\n",
    "# 4.1.1 - Instancia e treina Regressão Logística\n",
    "modelo_baseline = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    solver='liblinear'  # Adequado para datasets pequenos/médios\n",
    ")\n",
    "\n",
    "modelo_baseline.fit(X_train, y_train)\n",
    "\n",
    "# 4.1.2 - Realiza previsões\n",
    "y_pred_baseline = modelo_baseline.predict(X_test)\n",
    "y_pred_proba_baseline = modelo_baseline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Armazena resultados\n",
    "resultados_baseline = {\n",
    "    'modelo': modelo_baseline,\n",
    "    'y_pred': y_pred_baseline,\n",
    "    'y_pred_proba': y_pred_proba_baseline,\n",
    "    'y_true': y_test\n",
    "}\n",
    "\n",
    "print(f\"✓ Modelo de Regressão Logística treinado\")\n",
    "print(f\"✓ Previsões realizadas no conjunto de teste\")\n",
    "print(f\"✓ Amostras de treino: {len(X_train)}\")\n",
    "print(f\"✓ Amostras de teste: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Treinamento do Modelo Principal (XGBoost)\n",
    "\n",
    "### 4.2.1 - Instanciar um XGBClassifier\n",
    "### 4.2.2 - Configurar scale_pos_weight para desbalanceamento\n",
    "### 4.2.3 - Treinar o modelo\n",
    "### 4.2.4 - Realizar previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TREINAMENTO DO MODELO PRINCIPAL (XGBOOST) ===\")\n",
    "\n",
    "# Calcula scale_pos_weight para balanceamento\n",
    "contagem_classes = y_train.value_counts()\n",
    "scale_pos_weight = contagem_classes[0] / contagem_classes[1] if 1 in contagem_classes else 1\n",
    "\n",
    "print(f\"✓ Scale pos weight calculado: {scale_pos_weight:.4f}\")\n",
    "\n",
    "# 4.2.1 e 4.2.2 - Instancia XGBClassifier com configurações\n",
    "modelo_xgboost = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# 4.2.3 - Treina o modelo\n",
    "modelo_xgboost.fit(X_train, y_train)\n",
    "\n",
    "# 4.2.4 - Realiza previsões\n",
    "y_pred_xgb = modelo_xgboost.predict(X_test)\n",
    "y_pred_proba_xgb = modelo_xgboost.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Armazena resultados\n",
    "resultados_xgboost = {\n",
    "    'modelo': modelo_xgboost,\n",
    "    'y_pred': y_pred_xgb,\n",
    "    'y_pred_proba': y_pred_proba_xgb,\n",
    "    'y_true': y_test,\n",
    "    'feature_importance': modelo_xgboost.feature_importances_\n",
    "}\n",
    "\n",
    "print(f\"✓ Modelo XGBoost treinado\")\n",
    "print(f\"✓ Previsões realizadas no conjunto de teste\")\n",
    "print(f\"✓ Hiperparâmetros configurados para robustez\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Avaliação de Métricas de Desempenho\n",
    "\n",
    "### 4.3.1 - Calcular e analisar métricas para ambos os modelos\n",
    "- Matriz de Confusão\n",
    "- Precisão (Precision)\n",
    "- Revocação (Recall)\n",
    "- F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_metricas_desempenho(resultados, nome_modelo):\n",
    "    \"\"\"\n",
    "    Calcula e exibe métricas de desempenho para um modelo.\n",
    "    \"\"\"\n",
    "    y_true = resultados['y_true']\n",
    "    y_pred = resultados['y_pred']\n",
    "    \n",
    "    print(f\"\\n=== AVALIAÇÃO DE MÉTRICAS - {nome_modelo.upper()} ===\")\n",
    "    \n",
    "    # Matriz de Confusão\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Métricas\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    accuracy = (y_pred == y_true).mean()\n",
    "    \n",
    "    # Relatório detalhado\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    metricas = {\n",
    "        'matriz_confusao': cm,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report\n",
    "    }\n",
    "    \n",
    "    # Exibe resultados\n",
    "    print(f\"✓ Matriz de Confusão:\")\n",
    "    print(f\"   [[TN={cm[0,0]}, FP={cm[0,1]}],\")\n",
    "    print(f\"    [FN={cm[1,0]}, TP={cm[1,1]}]]\")\n",
    "    \n",
    "    print(f\"\\n✓ Métricas de Desempenho:\")\n",
    "    print(f\"   - Acurácia: {accuracy:.4f}\")\n",
    "    print(f\"   - Precisão: {precision:.4f}\")\n",
    "    print(f\"   - Recall: {recall:.4f}\")\n",
    "    print(f\"   - F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Interpretação das métricas\n",
    "    print(f\"\\n✓ Interpretação:\")\n",
    "    print(f\"   - Precisão: {precision:.2%} das previsões de 'alta' estão corretas\")\n",
    "    print(f\"   - Recall: {recall:.2%} dos dias de 'alta' foram identificados\")\n",
    "    print(f\"   - F1-Score: {f1:.4f} (média harmônica de precisão e recall)\")\n",
    "    \n",
    "    return metricas\n",
    "\n",
    "# Avalia modelo baseline\n",
    "metricas_baseline = avaliar_metricas_desempenho(resultados_baseline, \"Regressão Logística\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia modelo XGBoost\n",
    "metricas_xgboost = avaliar_metricas_desempenho(resultados_xgboost, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização das Matrizes de Confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_matriz_confusao(resultados, nome_modelo, salvar_grafico=False):\n",
    "    \"\"\"\n",
    "    Plota a matriz de confusão de forma visual.\n",
    "    \"\"\"\n",
    "    y_true = resultados['y_true']\n",
    "    y_pred = resultados['y_pred']\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "               xticklabels=['Baixa (0)', 'Alta (1)'],\n",
    "               yticklabels=['Baixa (0)', 'Alta (1)'])\n",
    "    plt.title(f'Matriz de Confusão - {nome_modelo}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predição', fontsize=12)\n",
    "    plt.ylabel('Valor Real', fontsize=12)\n",
    "    \n",
    "    if salvar_grafico:\n",
    "        plt.savefig(f'Tech Challenge 2/final/matriz_confusao_{nome_modelo.lower().replace(\" \", \"_\")}.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Matriz de confusão salva para {nome_modelo}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plota matrizes de confusão\n",
    "plotar_matriz_confusao(resultados_baseline, \"Regressão Logística\")\n",
    "plotar_matriz_confusao(resultados_xgboost, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Validação Walk-Forward\n",
    "\n",
    "### 4.4.1 - Implementar validação walk-forward com 3 dobras\n",
    "### 4.4.2 - Treinar XGBoost para cada dobra\n",
    "### 4.4.3 - Coletar métricas de cada dobra\n",
    "### 4.4.4 - Calcular média e desvio padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao_walk_forward(dados_preparados, n_splits=3):\n",
    "    \"\"\"\n",
    "    Implementa validação walk-forward simplificada.\n",
    "    \"\"\"\n",
    "    print(f\"=== VALIDAÇÃO WALK-FORWARD ({n_splits} DOBRAS) ===\")\n",
    "    \n",
    "    # Dados completos (treino + teste)\n",
    "    X_completo = pd.concat([\n",
    "        dados_preparados['X_train_scaled'], \n",
    "        dados_preparados['X_test_scaled']\n",
    "    ])\n",
    "    y_completo = pd.concat([\n",
    "        dados_preparados['y_train'], \n",
    "        dados_preparados['y_test']\n",
    "    ])\n",
    "    \n",
    "    # Configuração do TimeSeriesSplit\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    metricas_dobras = []\n",
    "    \n",
    "    for i, (train_idx, test_idx) in enumerate(tscv.split(X_completo)):\n",
    "        print(f\"\\n--- Dobra {i+1}/{n_splits} ---\")\n",
    "        \n",
    "        # Dados da dobra\n",
    "        X_train_fold = X_completo.iloc[train_idx]\n",
    "        y_train_fold = y_completo.iloc[train_idx]\n",
    "        X_test_fold = X_completo.iloc[test_idx]\n",
    "        y_test_fold = y_completo.iloc[test_idx]\n",
    "        \n",
    "        # Calcula scale_pos_weight para a dobra\n",
    "        contagem_classes = y_train_fold.value_counts()\n",
    "        scale_pos_weight = contagem_classes[0] / contagem_classes[1] if 1 in contagem_classes else 1\n",
    "        \n",
    "        # Treina modelo XGBoost para a dobra\n",
    "        modelo_fold = xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            random_state=42,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        modelo_fold.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Previsões\n",
    "        y_pred_fold = modelo_fold.predict(X_test_fold)\n",
    "        \n",
    "        # Métricas da dobra\n",
    "        precision_fold = precision_score(y_test_fold, y_pred_fold, average='binary')\n",
    "        recall_fold = recall_score(y_test_fold, y_pred_fold, average='binary')\n",
    "        f1_fold = f1_score(y_test_fold, y_pred_fold, average='binary')\n",
    "        accuracy_fold = (y_pred_fold == y_test_fold).mean()\n",
    "        \n",
    "        metricas_fold = {\n",
    "            'dobra': i+1,\n",
    "            'precision': precision_fold,\n",
    "            'recall': recall_fold,\n",
    "            'f1_score': f1_fold,\n",
    "            'accuracy': accuracy_fold,\n",
    "            'periodo_treino': f\"{X_train_fold.index.min()} até {X_train_fold.index.max()}\",\n",
    "            'periodo_teste': f\"{X_test_fold.index.min()} até {X_test_fold.index.max()}\",\n",
    "            'amostras_treino': len(X_train_fold),\n",
    "            'amostras_teste': len(X_test_fold)\n",
    "        }\n",
    "        \n",
    "        metricas_dobras.append(metricas_fold)\n",
    "        \n",
    "        print(f\"   Período treino: {metricas_fold['periodo_treino']}\")\n",
    "        print(f\"   Período teste: {metricas_fold['periodo_teste']}\")\n",
    "        print(f\"   F1-Score: {f1_fold:.4f}\")\n",
    "        print(f\"   Precisão: {precision_fold:.4f}\")\n",
    "        print(f\"   Recall: {recall_fold:.4f}\")\n",
    "    \n",
    "    return metricas_dobras\n",
    "\n",
    "# Executa validação walk-forward\n",
    "metricas_dobras = validacao_walk_forward(dados_preparados, n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula estatísticas agregadas\n",
    "df_metricas = pd.DataFrame(metricas_dobras)\n",
    "\n",
    "estatisticas_agregadas = {\n",
    "    'precision_media': df_metricas['precision'].mean(),\n",
    "    'precision_std': df_metricas['precision'].std(),\n",
    "    'recall_media': df_metricas['recall'].mean(),\n",
    "    'recall_std': df_metricas['recall'].std(),\n",
    "    'f1_score_media': df_metricas['f1_score'].mean(),\n",
    "    'f1_score_std': df_metricas['f1_score'].std(),\n",
    "    'accuracy_media': df_metricas['accuracy'].mean(),\n",
    "    'accuracy_std': df_metricas['accuracy'].std()\n",
    "}\n",
    "\n",
    "resultados_walk_forward = {\n",
    "    'metricas_por_dobra': metricas_dobras,\n",
    "    'estatisticas_agregadas': estatisticas_agregadas,\n",
    "    'dataframe_metricas': df_metricas\n",
    "}\n",
    "\n",
    "print(f\"\\n=== RESULTADOS AGREGADOS DA VALIDAÇÃO WALK-FORWARD ===\")\n",
    "print(f\"✓ F1-Score: {estatisticas_agregadas['f1_score_media']:.4f} ± {estatisticas_agregadas['f1_score_std']:.4f}\")\n",
    "print(f\"✓ Precisão: {estatisticas_agregadas['precision_media']:.4f} ± {estatisticas_agregadas['precision_std']:.4f}\")\n",
    "print(f\"✓ Recall: {estatisticas_agregadas['recall_media']:.4f} ± {estatisticas_agregadas['recall_std']:.4f}\")\n",
    "print(f\"✓ Acurácia: {estatisticas_agregadas['accuracy_media']:.4f} ± {estatisticas_agregadas['accuracy_std']:.4f}\")\n",
    "\n",
    "# Exibe tabela com resultados por dobra\n",
    "print(\"\\n=== MÉTRICAS POR DOBRA ===\")\n",
    "display(df_metricas[['dobra', 'precision', 'recall', 'f1_score', 'accuracy', 'amostras_treino', 'amostras_teste']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização dos Resultados da Validação Walk-Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras com as métricas por dobra\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Métricas por Dobra - Validação Walk-Forward', fontsize=16, fontweight='bold')\n",
    "\n",
    "metricas_plot = ['precision', 'recall', 'f1_score', 'accuracy']\n",
    "titulos = ['Precisão', 'Recall', 'F1-Score', 'Acurácia']\n",
    "\n",
    "for i, (metrica, titulo) in enumerate(zip(metricas_plot, titulos)):\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    bars = ax.bar(df_metricas['dobra'], df_metricas[metrica], \n",
    "                  color=sns.color_palette(\"husl\", len(df_metricas)), alpha=0.8)\n",
    "    \n",
    "    # Adiciona valores nas barras\n",
    "    for bar, valor in zip(bars, df_metricas[metrica]):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{valor:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Linha da média\n",
    "    media = df_metricas[metrica].mean()\n",
    "    ax.axhline(y=media, color='red', linestyle='--', alpha=0.7, \n",
    "               label=f'Média: {media:.3f}')\n",
    "    \n",
    "    ax.set_title(titulo, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Dobra')\n",
    "    ax.set_ylabel(titulo)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação Final dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabela comparativa final\n",
    "comparacao_final = pd.DataFrame({\n",
    "    'Modelo': ['Regressão Logística', 'XGBoost', 'XGBoost (Walk-Forward)'],\n",
    "    'Precisão': [\n",
    "        metricas_baseline['precision'],\n",
    "        metricas_xgboost['precision'],\n",
    "        estatisticas_agregadas['precision_media']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        metricas_baseline['recall'],\n",
    "        metricas_xgboost['recall'],\n",
    "        estatisticas_agregadas['recall_media']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        metricas_baseline['f1_score'],\n",
    "        metricas_xgboost['f1_score'],\n",
    "        estatisticas_agregadas['f1_score_media']\n",
    "    ],\n",
    "    'Acurácia': [\n",
    "        metricas_baseline['accuracy'],\n",
    "        metricas_xgboost['accuracy'],\n",
    "        estatisticas_agregadas['accuracy_media']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=== COMPARAÇÃO FINAL DOS MODELOS ===\")\n",
    "display(comparacao_final.round(4))\n",
    "\n",
    "# Identifica o melhor modelo\n",
    "melhor_f1 = comparacao_final.loc[comparacao_final['F1-Score'].idxmax()]\n",
    "print(f\"\\n✓ Melhor modelo por F1-Score: {melhor_f1['Modelo']} (F1: {melhor_f1['F1-Score']:.4f})\")\n",
    "\n",
    "print(\"\\n=== CONCLUSÕES ===\")\n",
    "print(\"✓ Fase 4 concluída com sucesso\")\n",
    "print(\"✓ Modelos treinados e avaliados\")\n",
    "print(\"✓ Validação walk-forward implementada\")\n",
    "print(\"✓ Métricas de robustez calculadas\")\n",
    "print(\"✓ Comparação entre modelos realizada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
